{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f07914",
   "metadata": {},
   "source": [
    "# P300 analysis from OpenVibe/BCI2000\n",
    "With lots of cool preprocessing features.\n",
    "source: [https://github.com/lokinou/p300_analysis_from_openvibe](https://github.com/lokinou/p300_analysis_from_openvibe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84d777ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the line wit qt below to obtain separate plots\n",
    "%matplotlib inline\n",
    "#%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d2b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if packages are missing, uncomment and execute here or in anaconda prompt with p300mne env\n",
    "#!pip install \"git+https://github.com/nbara/python-meegkit\"\n",
    "#!pip install statsmodels pyriemann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64932226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import mne\n",
    "import itertools\n",
    "import re\n",
    "from pathlib import Path\n",
    "# LDA\n",
    "from sklearn import metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866800c5",
   "metadata": {},
   "source": [
    "## Define the analysis variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffefc7d",
   "metadata": {},
   "source": [
    "if you don't know how to convert the .ov files, please check my [ov to gdf tutorial](https://github.com/lokinou/openvibe_to_gdf_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abb2827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the gdf files\n",
    "#data_dir=r\"C:\\BCI\\dev\\p300_analysis_from_openvibe\"\n",
    "data_dir=r\"./data_sample\"\n",
    "data_type= None  # bci2000 or openvibe or None for autodetection\n",
    "\n",
    "# Define the electrodes here (for the provided sample file)\n",
    "cname = None\n",
    "cname = ['Fz', 'FC1', 'FC2', 'C1', 'Cz', 'C2', 'P3', 'Pz', 'P4', 'Oz']\n",
    "#cname = ['Fz', 'Cz', 'P3', 'Pz', 'P4', 'PO7', 'PO8', 'Oz']\n",
    "\n",
    "# Visual\n",
    "skip_slow_ERP_plot = False  # skip the channelwise ERP plot\n",
    "display_preprocessing_plots = True\n",
    "display_all_erp_plots = True\n",
    "export_figures = True\n",
    "fig_folder = './out'\n",
    "\n",
    "# Preprocessing\n",
    "apply_resample = True # in case the sampling rate is high (>=256Hz)\n",
    "resample_freq = 256 # Hz\n",
    "apply_infinite_reference = False  # rereferencing\n",
    "apply_ASR = True  # use Artifact Subspace Reconstruction (artifact removal)\n",
    "apply_CSD = False  # use Current Source Density (spatial filter)\n",
    "\n",
    "drop_bad_epochs = False\n",
    "reject_channels_full_of_artifacts = True\n",
    "reject_artifactual_epochs = reject_channels_full_of_artifacts and True # do not reject epochs if you dont reject channels or use CSD\n",
    "artifact_threshold = 100e-6\n",
    "ratio_tolerated_artifacts = 0.3  # if 30% of artifacts in 200ms windows, then the channel is rejected\n",
    "\n",
    "# ERP analysis parameters (values in sec)\n",
    "pre_epoch = -.2\n",
    "epoch_length = .6\n",
    "baseline = (-.2, 0)\n",
    "#isi = .0625\n",
    "#flash = .125\n",
    "\n",
    "\n",
    "# LDA\n",
    "resample_LDA = 32 # Hz\n",
    "nb_k_splits = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041485e",
   "metadata": {},
   "source": [
    "## Load the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6eda1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BCI2kReader import BCI2kReader as b2k\n",
    "from BCI2kReader import FileReader as f2k\n",
    "\n",
    "raws = []\n",
    "\n",
    "def extract_annotations(filename, verbose=False):\n",
    "    display_preprocessing_plots = False\n",
    "    file= b2k.BCI2kReader(filename)\n",
    "\n",
    "    if verbose:\n",
    "        print(file.states)\n",
    "    target_states = np.squeeze(file.states['StimulusType'])\n",
    "    stimulus_codes = np.squeeze(file.states['StimulusCode'])\n",
    "    if 'StimulusBegin' in file.states.keys():\n",
    "        stimulus_begin = np.squeeze(file.states['StimulusBegin'])\n",
    "    else:\n",
    "        stimulus_begin = np.squeeze(file.states['Flashing'])\n",
    "\n",
    "    phase = np.squeeze(file.states['PhaseInSequence'])\n",
    "\n",
    "    fs = file.samplingrate\n",
    "\n",
    "    idx_targets = np.where(target_states)[0]\n",
    "    idx_codes = np.where(stimulus_codes>0)[0]\n",
    "    idx_begin = np.where(stimulus_begin>0)[0]\n",
    "\n",
    "\n",
    "    # In BCI2000 states are maintained over different samples, we search here the differences of when the codes are > 0\n",
    "    groups = np.split(idx_codes, np.where(np.diff(idx_codes) != 1)[0]+1)\n",
    "    # we take the first sample where a difference can be found\n",
    "    code_change_idx = np.array([g[0] for g in groups])\n",
    "    #[idx_codes[idx] for idx in code_change_idx]\n",
    "    print('nb stimuli={}'.format(len(code_change_idx)))\n",
    "\n",
    "    # we intersect the target index list with the code change to find the onset of targets and non-targets\n",
    "    target_idx=np.intersect1d(code_change_idx,idx_targets)\n",
    "    print('nb targets={}'.format(len(target_idx)))\n",
    "    non_target_idx= np.setdiff1d(code_change_idx,idx_targets)\n",
    "\n",
    "    # Translating into MNE Annotations \n",
    "    # define the annotations from the recovered stimuli (in seconds)\n",
    "    sample_lengh = 1/fs\n",
    "    onsets = code_change_idx * sample_lengh\n",
    "    onsets = np.repeat(onsets, 2)  # repeat onsets\n",
    "    # define the descriptio\n",
    "    description_targets = np.zeros(code_change_idx.shape, dtype=np.uint)\n",
    "    # index of targets in the list of stimuli onsets\n",
    "    description_targets[np.searchsorted(code_change_idx, target_idx)] = 1\n",
    "    description_codes = stimulus_codes[code_change_idx] + 100  # start codes at 100 because 0 and 1 are used for target and nontarget\n",
    "    # merge code and target decriptions\n",
    "    description = np.zeros(description_targets.shape[0]*2, dtype=np.uint)\n",
    "    description[np.arange(description_targets.shape[0]*2, step=2)] = description_codes\n",
    "    description[np.arange(start=1, stop=(description_targets.shape[0]*2)+1, step=2)] = description_targets\n",
    "\n",
    "    if display_preprocessing_plots:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(description[:100])\n",
    "        fig.suptitle('Targets(1) and non-targets(0) for 100 first stimuli')\n",
    "\n",
    "    if display_preprocessing_plots:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(phase == 1)\n",
    "        fig.suptitle('Trial begin')\n",
    "\n",
    "    # extract trial begin markers\n",
    "    new_phase_continuous = np.where(phase == 1)[0]\n",
    "    groups = np.split(new_phase_continuous, np.where(np.diff(new_phase_continuous) != 1)[0]+1)\n",
    "    new_trial_idx = np.array([g[0] for g in groups])\n",
    "    # extract trial end markers\n",
    "    new_phase_continuous = np.where(phase == 3)[0]\n",
    "    groups = np.split(new_phase_continuous, np.where(np.diff(new_phase_continuous) != 1)[0]+1)\n",
    "    end_of_trial_idx = np.array([g[-1] for g in groups]) # take the last index to integrate all post sequence duration\n",
    "\n",
    "    if new_trial_idx.shape[0] > end_of_trial_idx[0]:\n",
    "        print('WARNING: no end of trial for the last trial (interrupted recording?), it will be ignored for offline accuracy calculation')\n",
    "        inter_trial_duration = end_of_trial_idx[0:len(new_trial_idx)] - new_trial_idx\n",
    "    else:\n",
    "        inter_trial_duration = end_of_trial_idx - new_trial_idx\n",
    "    inter_trial_duration = inter_trial_duration * sample_lengh  # express in seconds\n",
    "\n",
    "\n",
    "    print(\"Extracted {} trials\".format(len(new_trial_idx)))\n",
    "\n",
    "    # set a non-zero duration for stimuli (or MNE ignores them)\n",
    "    duration = np.ones(onsets.shape) * sample_lengh\n",
    "\n",
    "\n",
    "    # merge phase in sequence events with stimuli onsets\n",
    "    onsets_phase = new_trial_idx * sample_lengh\n",
    "    onsets = np.concatenate((onsets, onsets_phase))\n",
    "    duration = np.concatenate((duration, inter_trial_duration))\n",
    "    description = np.concatenate((description, np.ones(new_trial_idx.shape) * 10))  # concatenate trials markers=10\n",
    "    srt = np.argsort(onsets) # sort according to their timing\n",
    "    duration = duration[srt]\n",
    "    description = description[srt]\n",
    "    inter_trial_duration\n",
    "    annotations = mne.Annotations(onset=onsets, duration=duration, description=description)\n",
    "\n",
    "    file.flush()\n",
    "    return annotations\n",
    "\n",
    "def load_bci2k(filename_list):\n",
    "    raws = []\n",
    "    for fn in filename_list:\n",
    "        cname = None\n",
    "        with b2k.BCI2kReader(fn) as file:\n",
    "            \n",
    "            # Extract signals and states\n",
    "            print('opened')\n",
    "            eeg_data = file.signals\n",
    "            states = file.states\n",
    "            fs = file.samplingrate\n",
    "            nb_chan = eeg_data.shape[0]\n",
    "            #file.purge()\n",
    "\n",
    "            # Extract channel names\n",
    "            reader = f2k.bcistream(fn)\n",
    "            # actualize the parameters by including the defined channel names\n",
    "            if len(reader.params['ChannelNames']):\n",
    "                if cname != reader.params['ChannelNames']:\n",
    "                    cname = reader.params['ChannelNames']\n",
    "                    print('Actualized channel names to {}'.format(cname))\n",
    "\n",
    "            if cname is None:\n",
    "                cname = [str(ch_n) for ch_n in list(range(nb_chan))]\n",
    "                \n",
    "            # convert states into annotations\n",
    "            info = mne.create_info(cname, fs, ch_types='eeg', verbose=None)\n",
    "            raw_array = mne.io.RawArray(eeg_data, info)\n",
    "            # Manually force the filename or mne complains\n",
    "            raw_array._filenames = [os.path.basename(fn)]\n",
    "            \n",
    "            annotations = extract_annotations(fn)\n",
    "            raw_array.set_annotations(annotations)\n",
    "            raws.append(raw_array)\n",
    "    return raws\n",
    "\n",
    "#fn = [\"./data_sample/bci2000\\Heide_einsteinBP_calibration4S001R01.dat\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1a34fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_sample\\bp2calib.dat\n",
      "opened\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating RawArray with float64 data, n_channels=10, n_times=57824\n",
      "    Range : 0 ... 57823 =      0.000 ...   225.871 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: failed to interpret \"auto\" as type int in parameter \"VisualizeSourceBufferSize\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb stimuli=1050\n",
      "nb targets=150\n",
      "Extracted 5 trials\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-e6d63db0218f>:134: RuntimeWarning: Limited 1 annotation(s) that were expanding outside the data range.\n",
      "  raw_array.set_annotations(annotations)\n"
     ]
    }
   ],
   "source": [
    "os.path.exists(data_dir)\n",
    "fnames = []\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".gdf\"):\n",
    "        data_type = 'openvibe'\n",
    "        fnames.append(os.path.join(data_dir, file))\n",
    "        print(os.path.join(data_dir, file))\n",
    "    elif file.endswith(\".dat\"):\n",
    "        data_type = 'bci2000'\n",
    "        print(os.path.join(data_dir, file))\n",
    "        fnames.append(os.path.join(data_dir, file))\n",
    "        \n",
    "if data_type == 'openvibe':\n",
    "    # load and preprocess data ####################################################\n",
    "    raws = [mne.io.read_raw_gdf(f, preload=True) for f in fnames]\n",
    "elif data_type == 'bci2000':\n",
    "    raws = load_bci2k(fnames)\n",
    "raw = mne.concatenate_raws(raws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29780a12-ad87-46f7-b00a-8992734229e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeb81d-d08c-4a59-a013-fe1ecb6dda0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f9a3b-9862-47d9-9eb4-f3e618e76379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58513759-e223-4bd4-8f6d-9da09a7b1e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_targets.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "471aa253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaled signal to Volt (mean variance=2.749515417312947e-10)\n"
     ]
    }
   ],
   "source": [
    "# If the variance of the data is >1, it means the data is expressed in microvolts\n",
    "# Since MNE uses Volt as a default value, we rescale microvolts to volt\n",
    "if np.var(raw._data)>1:\n",
    "    raw._data = raw._data * 1.e-6\n",
    "    print('Rescaled signal to Volt (mean variance={})'.format(np.var(raw._data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518983d",
   "metadata": {},
   "source": [
    "Create a name for figures output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eaf9aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures will have the name: bp2calib\n"
     ]
    }
   ],
   "source": [
    "output_name = Path(raw._filenames[0]).stem\n",
    "if len(raw._filenames)>1:\n",
    "    output_name = output_name + '_{}_files'.format(len(raw._filenames))\n",
    "print('Figures will have the name: {}'.format(output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4461997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_preprocessing_plots:\n",
    "    raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97b8f2",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b534c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_resample:\n",
    "    raw.resample(resample_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "60242a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using user defined channel names: ['Fz', 'FC1', 'FC2', 'C1', 'Cz', 'C2', 'P3', 'Pz', 'P4', 'Oz']\n",
      "Electrode mapping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Fz': 'eeg',\n",
       " 'FC1': 'eeg',\n",
       " 'FC2': 'eeg',\n",
       " 'C1': 'eeg',\n",
       " 'Cz': 'eeg',\n",
       " 'C2': 'eeg',\n",
       " 'P3': 'eeg',\n",
       " 'Pz': 'eeg',\n",
       " 'P4': 'eeg',\n",
       " 'Oz': 'eeg'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montage = mne.channels.make_standard_montage('standard_1005')\n",
    "\n",
    "if cname is None:\n",
    "    cname = raw.info['ch_names']\n",
    "    print('Using channel names directly from the data files: {}'.format(cname))\n",
    "else:\n",
    "    print('Using user defined channel names: {}'.format(cname))\n",
    "        \n",
    "\n",
    "nb_chan = len(raw.info['ch_names'])\n",
    "nb_def_ch = len(cname)\n",
    "\n",
    "# regular expressions to look for certain channel types\n",
    "eog_pattern = re.compile('eog|EOG')  # EOG electrodes\n",
    "emg_pattern = re.compile('emg|EMG')  # EMG electrodes\n",
    "mastoid_pattern = re.compile('^[aA][0-9]+')  # A1 and A2 electrodes (mastoids)\n",
    "\n",
    "\n",
    "types = []\n",
    "cname_map = dict(zip(raw.info['ch_names'], cname))\n",
    "for nc in cname:\n",
    "    if eog_pattern.search(nc) is not None:\n",
    "        t = 'eog'\n",
    "    elif emg_pattern.search(nc) is not None:\n",
    "        t = 'emg'\n",
    "    elif mastoid_pattern.search(nc) is not None:\n",
    "        t = 'misc'\n",
    "    elif nc in montage.ch_names:  # check the 10-05 montage for all electrode names\n",
    "        t = 'eeg'\n",
    "    else:\n",
    "        t = 'misc'  # if not found, discard the channel as misc\n",
    "    \n",
    "    types.append(t)\n",
    "        \n",
    "type_map = dict(zip(cname, types))\n",
    "\n",
    "# rename and pick eeg\n",
    "raw.rename_channels(cname_map, allow_duplicates=False)\n",
    "raw.set_channel_types(type_map)\n",
    "raw.pick_types(eeg=True, misc=False)\n",
    "print('Electrode mapping')\n",
    "type_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475591f",
   "metadata": {},
   "source": [
    "rename the electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdc3cc",
   "metadata": {},
   "source": [
    "Set the montage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "46773eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.set_montage(montage, match_case=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199c75a",
   "metadata": {},
   "source": [
    "Check whether there are bad channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ef84aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective window size : 1.000 (s)\n",
      "50Hz variance: []\n",
      "Variance: []\n"
     ]
    }
   ],
   "source": [
    "# Using 50Hz power variance\n",
    "psd, freqs = mne.time_frequency.psd_welch(raw,verbose=True)\n",
    "power_50hz = psd[:,np.where(freqs ==60)]\n",
    "print('50Hz variance: {}'.format(mne.preprocessing.bads._find_outliers(power_50hz.squeeze(), threshold=3, max_iter=5, tail=0)))\n",
    "\n",
    "\n",
    "# using variance\n",
    "ch_var  = [np.var(raw._data[i,:]) for i in list(range(raw._data.shape[0]))]\n",
    "print('Variance: {}'.format(mne.preprocessing.bads._find_outliers(ch_var, threshold=3, max_iter=5, tail=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e9f7e",
   "metadata": {},
   "source": [
    "rereferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a932dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_infinite_reference:\n",
    "    raw.del_proj()  # remove our average reference projector first\n",
    "    sphere = mne.make_sphere_model('auto', 'auto', raw.info)\n",
    "    src = mne.setup_volume_source_space(sphere=sphere, exclude=30., pos=15.)\n",
    "    forward = mne.make_forward_solution(raw.info, trans=None, src=src, bem=sphere)\n",
    "    raw_rest = raw.copy().set_eeg_reference('REST', forward=forward)\n",
    "    \n",
    "    if display_preprocessing_plots:\n",
    "        for title, _raw in zip(['Original', 'REST (∞)'], [raw, raw_rest]):\n",
    "            fig = _raw.plot(n_channels=len(raw), scalings=dict(eeg=5e-5))\n",
    "            # make room for title\n",
    "            fig.subplots_adjust(top=0.9)\n",
    "            fig.suptitle('{} reference'.format(title), size='xx-large', weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ce84f",
   "metadata": {},
   "source": [
    "## Bandpass the signal\n",
    "Removes noise and drift from the EEG signal by applying a infinite impulse response (two-pass) filter between .5 and 40Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3dfb0b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.5 - 40 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth bandpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 16 (effective, after forward-backward)\n",
      "- Cutoffs at 0.50, 40.00 Hz: -6.02, -6.02 dB\n",
      "\n",
      "Setting up band-stop filter from 49 - 51 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 49.38\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 49.12 Hz)\n",
      "- Upper passband edge: 50.62 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 50.88 Hz)\n",
      "- Filter length: 1691 samples (6.605 sec)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw.filter(.5, 40, fir_window='hann', method='iir')\n",
    "raw.notch_filter(50)  # removes 50Hz noise\n",
    "if display_preprocessing_plots:\n",
    "    raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224b322",
   "metadata": {},
   "source": [
    "## Excluding of channels full of artifacts (muscular or disconnecting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1dd7005e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "Not setting metadata\n",
      "2257 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Loading data for 2257 events and 51 original time points ...\n",
      "0 bad epochs dropped\n",
      "(2257, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "#Apply a variance based channel rejection if artifacts are present >30% of the time\n",
    "def detec_rej_channel(raw, duration=.2, overlap_duration=.1, threshold_eeg=artifact_threshold, reject_ratio=ratio_tolerated_artifacts):\n",
    "\n",
    "    epochs_rej = mne.make_fixed_length_epochs(raw,duration=duration, overlap=overlap_duration, preload=True)\n",
    "    epochs_rej._data.shape\n",
    "    diff = np.max(epochs_rej._data, axis=2) - np.min(epochs_rej._data, axis=2)\n",
    "\n",
    "    print(diff.shape)\n",
    "\n",
    "    rej = (diff>=threshold_eeg).astype(np.float64)\n",
    "    rel = sns.heatmap(rej)\n",
    "    rel.set(title='Detected artifacts per electrode and runs (white)')\n",
    "\n",
    "    # calculate ratio of rejected trials\n",
    "    ratios = np.sum(rej,axis=0) / rej.shape[0]\n",
    "    \n",
    "    ret = np.argwhere(ratios >= reject_ratio).tolist()\n",
    "    if len(ret)>0 and len(ret[0]):\n",
    "        print('Found {} channels with at least {}% {}s epochs > {} amplitude)'.format(len(ret), \n",
    "                                                                                              reject_ratio*100, duration,\n",
    "                                                                                      threshold_eeg))\n",
    "        return ret[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "if reject_channels_full_of_artifacts:\n",
    "    rej_ch = detec_rej_channel(raw)\n",
    "    if rej_ch is not None:\n",
    "        new_bads = [raw.info['ch_names'][ch] for ch in rej_ch]\n",
    "        raw.info['bads'].extend(new_bads)\n",
    "        raw.pick_types(eeg=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563dccf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Artifact Subspace Reconstruction fitting and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67c5a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on samples of size (10, 57824)\n",
      "Model trained\n"
     ]
    }
   ],
   "source": [
    "if apply_ASR:\n",
    "    #!pip install meegkit pymanopt\n",
    "    from meegkit.asr import ASR\n",
    "    fs = int(raw.info[\"sfreq\"])  # sampling frequency\n",
    "    method='riemann'  # if error, use 'euclid'\n",
    "    method='euclid' # pymanopt library still buggy\n",
    "    window_s=.5  # .5 sec window of analysis\n",
    "    data_interval_s  = None # (begin, end) in sec of the training sample\n",
    "    estimator='lwf'  #leave blank if using euclidian mode \n",
    "\n",
    "    # define the ASR model using riemannian method\n",
    "    #asr_model = ASR(sfreq=fs, method=method, win_len=window_s, estimator=estimator)\n",
    "\n",
    "    # if failing (after trying twice. SVD error occurs for no reason sometimes)\n",
    "    asr_model = ASR(sfreq=fs, method=method, win_len=window_s)\n",
    "\n",
    "    # The best would be to choose another recording during the same session to train the model without overfitting\n",
    "    data = raw._data  # the numpy array with data is stored in the _data variable\n",
    "\n",
    "    # Select a time interval for training data\n",
    "    train_idx = None\n",
    "    if data_interval_s is not None:\n",
    "        train_idx = np.arange(data_interval_s[0] * fs, data_interval_s[1] * fs, dtype=int)\n",
    "    # otherwise select the whole training set\n",
    "    else:\n",
    "        train_idx = np.arange(0, data.shape[1])\n",
    "\n",
    "    train_data = data[:, train_idx]\n",
    "    print('Training on samples of size {}'.format(train_data.shape))\n",
    "\n",
    "    # fir the ASR model with data intervals\n",
    "    _, sample_mask = asr_model.fit(train_data)\n",
    "    print('Model trained')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62943a",
   "metadata": {},
   "source": [
    "### Clean the current dataset\n",
    "Please check whether using this artifact filtering method increases signal to noise ratio rather than reducing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9282725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_ASR:\n",
    "    clean =  asr_model.transform(raw._data)\n",
    "\n",
    "    display_window_s = 60  # \n",
    "\n",
    "    if display_preprocessing_plots:  #\n",
    "        data_p = raw._data[0:fs*display_window_s]  # reshape to (n_chans, n_times)\n",
    "        clean_p = clean[0:fs*display_window_s]\n",
    "\n",
    "        ###############################################################################\n",
    "        # Plot the results\n",
    "        # -----------------------------------------------------------------------------\n",
    "        #\n",
    "        # Data was trained on a 40s window from 5s to 45s onwards (gray filled area).\n",
    "        # The algorithm then removes portions of this data with high amplitude\n",
    "        # artifacts before running the calibration (hatched area = good).\n",
    "        nb_ch_disp = len(raw.info['ch_names'])\n",
    "        times = np.arange(data_p.shape[-1]) / fs\n",
    "        f, ax = plt.subplots(nb_ch_disp, sharex=True, figsize=(32, 16))\n",
    "        for i in range(nb_ch_disp):\n",
    "            # ax[i].fill_between(train_idx / fs, 0, 1, color='grey', alpha=.3,\n",
    "            #                   transform=ax[i].get_xaxis_transform(),\n",
    "            #                   label='calibration window')\n",
    "            # ax[i].fill_between(train_idx / fs, 0, 1, where=sample_mask.flat,\n",
    "            #                   transform=ax[i].get_xaxis_transform(),\n",
    "            #                   facecolor='none', hatch='...', edgecolor='k',\n",
    "            #                   label='selected window')\n",
    "            ax[i].plot(times, data_p[i], lw=.5, label='before ASR')\n",
    "            ax[i].plot(times, clean_p[i], label='after ASR', lw=.5)\n",
    "            # ax[i].plot(times, raw[i]-clean[i], label='Diff', lw=.5)\n",
    "            # ax[i].set_ylim([-50, 50])\n",
    "            ax[i].set_ylabel(f'ch{i}')\n",
    "            ax[i].set_yticks([])\n",
    "        ax[i].set_xlabel('Time (s)')\n",
    "        ax[0].legend(fontsize='small', bbox_to_anchor=(1.04, 1), borderaxespad=0)\n",
    "        plt.subplots_adjust(hspace=0, right=0.75)\n",
    "        plt.suptitle('Before/after ASR')\n",
    "        plt.show()\n",
    "    raw.data_ = clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a02ab2",
   "metadata": {},
   "source": [
    "### Convert text annotations (i.e. unprocessed events) into events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61c1cd",
   "metadata": {},
   "source": [
    "**Small but major hack to realign events due to conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "51ba042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type=='openvibe':\n",
    "    print(\"Erroneous annotations: {}\".format(raw.annotations.description))\n",
    "    print('Note here that the first annotation is 0 or 1, this is an error and thus we shift the annotations to retrieve the correct timings')\n",
    "    raw.annotations.description = np.roll(raw.annotations.description, -1)\n",
    "    print(\"Corrected annotations: {}\".format(raw.annotations.description))\n",
    "\n",
    "    # in case you want to debug the issue, I left here a way to visualize them\n",
    "    # retrieving the list of annotations\n",
    "    import pprint\n",
    "    print(raw.annotations.to_data_frame())\n",
    "    df = raw.annotations.to_data_frame()\n",
    "    print('Displaying all annotations')\n",
    "    annot_codes = [np.int64(n) for n in np.unique(df['description'])]\n",
    "    df['description'] = df['description'].astype(int)\n",
    "\n",
    "    if False:\n",
    "        # to see and debug the fill list of annotations\n",
    "        import pandas as pd\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        #a = df[df['description'] != 33286]\n",
    "        #print(a)\n",
    "        print(df)\n",
    "        pd.set_option('display.max_rows', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd225eb",
   "metadata": {},
   "source": [
    "### Make a list of the annotations to check whether all stimuli can be found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f06080",
   "metadata": {
    "tags": []
   },
   "source": [
    "These annotations seem to relate to hex codes. OpenViBE definitions can be found on [OpenViBE's website](http://openvibe.inria.fr/stimulation-codes/). Let's parse the copypasted list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc165b",
   "metadata": {},
   "source": [
    "Make a dataframe of the stimuli in common between both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c3d00367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "if data_type=='openvibe':\n",
    "    tr_sim= ''\n",
    "    pat_extract= re.compile('^([^ ]+)[ ]+0x[0-9A-Fa-f]+[ \\/]+([0-9]+)')\n",
    "    #OVTK_GDF_125_Watt                                     0x585       //  1413\n",
    "    k_stim = []\n",
    "    k_stim_int = []\n",
    "    v_stim = []\n",
    "\n",
    "    # read and convert annotations\n",
    "    with open(r'.\\ov_stims.txt', 'r') as fd:\n",
    "        for line in fd.readlines():\n",
    "            m = pat_extract.match(line)\n",
    "            v, k = m.groups()\n",
    "            k_stim.append(k)\n",
    "            k_stim_int.append(int(k))\n",
    "            v_stim.append(v)\n",
    "\n",
    "    # format dict and list\n",
    "    stim_map = dict(zip(k_stim_int, v_stim))\n",
    "    stim_map_inv = dict(zip(v_stim, k_stim))\n",
    "\n",
    "    stim_tup = list(zip(k_stim_int, v_stim))\n",
    "\n",
    "    df = pd.DataFrame.from_dict(stim_tup)\n",
    "    df.columns = ['coden', 'desc']\n",
    "    df[[c in annot_codes for c in df.coden]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfa34d",
   "metadata": {},
   "source": [
    "From this table, we could locate and save the codes for **Target and Non-Target** and give them the following values: target=1 and non-target=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "686637bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = 1, nontarget = 0\n",
    "target_map = None\n",
    "if data_type == 'openvibe':\n",
    "    target_map = {'33286':0, '33285':1}\n",
    "elif data_type == \"bci2000\":\n",
    "    target_map = {'0':0, '1':1, '10':10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d6e4e",
   "metadata": {},
   "source": [
    "Then we can convert annotations into events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "24830d2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not find any of the events you specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-495811b6dcfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevents_from_annotations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_map\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Found {} events\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mevents_from_annotations\u001b[1;34m(raw, event_id, regexp, use_rounding, chunk_duration, verbose)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\p300mne\\lib\\site-packages\\mne\\annotations.py\u001b[0m in \u001b[0;36mevents_from_annotations\u001b[1;34m(raw, event_id, regexp, use_rounding, chunk_duration, verbose)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     \u001b[0mevent_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_event_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m     event_sel, event_id_ = _select_annotations_based_on_description(\n\u001b[0m\u001b[0;32m   1168\u001b[0m         annotations.description, event_id=event_id, regexp=regexp)\n\u001b[0;32m   1169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\p300mne\\lib\\site-packages\\mne\\annotations.py\u001b[0m in \u001b[0;36m_select_annotations_based_on_description\u001b[1;34m(descriptions, event_id, regexp)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_sel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mregexp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Could not find any of the events you specified.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mevent_sel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_id_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Could not find any of the events you specified."
     ]
    }
   ],
   "source": [
    "events, _ = mne.events_from_annotations(raw, event_id=target_map)\n",
    "print(\"Found {} events\".format(len(events[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1347e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024a2da",
   "metadata": {},
   "source": [
    "### Pick the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick all channels\n",
    "picks = mne.pick_channels(raw.info[\"ch_names\"], include=[])\n",
    "picks\n",
    "raw.plot_sensors(show_names=True)\n",
    "fig = raw.plot_sensors('3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc1d00",
   "metadata": {},
   "source": [
    "## Epoching from events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65f7ee",
   "metadata": {},
   "source": [
    "Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "events[:, 0]\n",
    "a = np.array(events[:, 0])\n",
    "dups = [item for item, count in Counter(a).items() if count > 1]\n",
    "if dups:\n",
    "    print(\"WARNING: Duplicate found at sample(s) {}\".format(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beabe92",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_ids = dict(NonTarget=0, Target=1) \n",
    "#isi = isi\n",
    "#flash = flash\n",
    "pre_epoch = pre_epoch\n",
    "epoch_length = epoch_length\n",
    "\n",
    "\n",
    "# epoching function\n",
    "epochs = mne.Epochs(raw, events, baseline=baseline, event_id=event_ids, tmin=pre_epoch, tmax=epoch_length, event_repeated='drop', picks = ['eeg', 'csd'],\n",
    "                    preload=True)\n",
    "\n",
    "# if there is any delay,\n",
    "#epochs.shift_time(-isi, relative=True)\n",
    "if display_preprocessing_plots:\n",
    "    fig = epochs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a91f1a",
   "metadata": {},
   "source": [
    "### Making a cross correlation plot between the electrodes to see how channels relate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ff9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.corrcoef(raw._data)\n",
    "fig = plt.figure()\n",
    "hm = sns.heatmap(m,linewidths=0,cmap=\"YlGnBu\").set(title='Cross correlation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced88097",
   "metadata": {},
   "source": [
    "### Epoch rejection\n",
    "Please filter out channels before epochs. A problematic channel can discard the whole recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c142c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if reject_artifactual_epochs:\n",
    "    reject_criteria = dict(eeg=150e-6)  # 100 µV  #eog=200e-6)\n",
    "    _ = epochs.drop_bad(reject=reject_criteria)\n",
    "    if display_preprocessing_plots:\n",
    "        epochs.plot_drop_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30c19a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply current source density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3abf42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if apply_CSD:\n",
    "    epochs_csd = mne.preprocessing.compute_current_source_density(epochs)\n",
    "    epochs = epochs_csd\n",
    "    if display_preprocessing_plots:\n",
    "        epochs_csd.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46a29b",
   "metadata": {},
   "source": [
    "### Average the epochs of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_nt = epochs['NonTarget'].average()\n",
    "l_target = epochs['Target'].average()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6729d3",
   "metadata": {},
   "source": [
    "target and non target signal plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    fig1 = l_target.plot(spatial_colors=True, axes=ax[0], show=False)\n",
    "    fig2 = l_nt.plot(spatial_colors=True, axes=ax[1], show=False)\n",
    "    # Add title\n",
    "    fig.suptitle(\"Target(top) - Non-Target(bottom)\")\n",
    "    # Fix font spacing\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57d133",
   "metadata": {},
   "source": [
    "target and non target signal topomaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf7661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    spec_kw = dict(width_ratios=[1,1,1,.15], wspace=0.5,\n",
    "                   hspace=0.5,height_ratios=[1,1])\n",
    "                             #hspace=0.5, height_ratios=[1, 2])\n",
    "\n",
    "    fig, ax = plt.subplots(2, 4, gridspec_kw=spec_kw)\n",
    "    l_target.plot_topomap(times=[0, 0.18, 0.4], average=0.05, axes=ax[0,:], show=False)\n",
    "    l_nt.plot_topomap(times=[0, 0.18, 0.4], average=0.05, axes=ax[1,:], show=False)\n",
    "    fig.suptitle(\"Target(top) - Non-Target(bottom)\")\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a88e9",
   "metadata": {},
   "source": [
    "joint plot (of the two former graphs). Plase not that Y scales differ between plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05494451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_target.plot_joint()\n",
    "plt.gcf().canvas.set_window_title('Target joint plot')\n",
    "l_nt.plot_joint()\n",
    "plt.gcf().canvas.set_window_title('Non-Target joint plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1c6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Average plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f086da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "                   Target=list(epochs['Target'].iter_evoked()))\n",
    "    #picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "    mne.viz.plot_compare_evokeds(evokeds, picks=picks, combine='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fb9b1",
   "metadata": {},
   "source": [
    "### Target vs NonTarget Erps per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ddccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_slow_ERP_plot:\n",
    "    nb_chans = epochs['Target']._data.shape[1]\n",
    "    splt_width = int(np.floor(np.sqrt(1.0*nb_chans+1)))  # adding an extra plot with all channels combined at the end\n",
    "    splt_height = splt_width if splt_width * splt_width >= nb_chans+1 else splt_width+1\n",
    "    if splt_height * splt_width < nb_chans+1:\n",
    "        splt_height += 1\n",
    "    fig, ax = plt.subplots(splt_height,splt_width)\n",
    "\n",
    "    evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "                   Target=list(epochs['Target'].iter_evoked()))\n",
    "    #picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "\n",
    "    shape_epochs = epochs['Target']._data.shape\n",
    "    for ch_idx in range(nb_chans):\n",
    "        print('plotting channel {}'.format(ch_idx+1))\n",
    "        mne.viz.plot_compare_evokeds(evokeds,picks=[epochs.info['ch_names'][ch_idx]],\n",
    "                                     legend=False,\n",
    "                                     axes=ax[ch_idx//splt_width, ch_idx%splt_width], show=False)\n",
    "        #plt.show(block=False)\n",
    "        plt.subplots_adjust(hspace=0.5, wspace=.5)\n",
    "        #plt.pause(.1)\n",
    "    print('plotting averaged channels')\n",
    "    axs = mne.viz.plot_compare_evokeds(evokeds, picks=picks, combine='mean',\n",
    "                                 legend=True,\n",
    "                                 axes=ax[-1,-1], show=False)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=.5)\n",
    "    plt.show()\n",
    "    print(\"Please note that this plot is optimized for higher resolution and has the legend overlapping the average\")\n",
    "\n",
    "    if export_figures:\n",
    "        out_name = os.path.join(fig_folder, output_name + '_ERPs')\n",
    "        axs[0].savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7930da8",
   "metadata": {},
   "source": [
    "### Display epoch at Cz and Pz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fdec8",
   "metadata": {},
   "source": [
    "### Display single epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd310e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    epochs['Target'].plot_image(combine='mean')\n",
    "    plt.gcf().canvas.set_window_title('Target')\n",
    "    epochs['NonTarget'].plot_image(combine='mean')\n",
    "    plt.gcf().canvas.set_window_title('Non-Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c0e3b",
   "metadata": {},
   "source": [
    "### Same plot but channel wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_slow_ERP_plot:\n",
    "    dict_electrodes = dict(eeg='EEG') if not apply_CSD else dict(csd='CSD')\n",
    "    if display_all_erp_plots:\n",
    "        for ch_type, title in dict_electrodes.items():\n",
    "            layout = mne.channels.find_layout(epochs.info, ch_type=ch_type)\n",
    "            epochs['Target'].plot_topo_image(layout=layout, fig_facecolor='w',\n",
    "                                                    font_color='k', title=title+' Target Trial x time amplitude')\n",
    "            epochs['NonTarget'].plot_topo_image(layout=layout, fig_facecolor='w',\n",
    "                                                    font_color='k', title=title+' Non-Target Trial x time amplitude')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4d9f1",
   "metadata": {},
   "source": [
    "# Assess ERP classificaiton accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5057efa",
   "metadata": {},
   "source": [
    "resample the signal, we don't need that much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a299028",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fs = resample_LDA #\n",
    "epochs_resampled = epochs.copy().resample(new_fs)\n",
    "print('resampling to {}Hz'.format(new_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2f656",
   "metadata": {},
   "source": [
    "modify the data matrix to be properly assessed via LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X = epochs_resampled._data[:,1,:]  # input data at CZ (TODO:flatten all electrodes)\n",
    "X = epochs_resampled._data[:,:,:]  # input data at CZ (TODO:flatten all electrodes)\n",
    "y = epochs_resampled.events[:,2]  # ground truth\n",
    "\n",
    "# remove the information \n",
    "    \n",
    "#mne.stats.permutation_t_test()\n",
    "print('Data shape from MNE {}'.format(X.shape))\n",
    "X = np.moveaxis(X,1,-1)\n",
    "print('new data shape with sampling prioritized over channels {}'.format(X.shape))\n",
    "X = X.reshape([X.shape[0],X.shape[1]*X.shape[2]],order='C')\n",
    "print('Shape for K-fold LDA {}'.format(X.shape))\n",
    "\n",
    "# Think about splitting training sample and test samples\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d2a1d",
   "metadata": {},
   "source": [
    "### Compute k-fold LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='lsqr',shrinkage='auto')\n",
    "kf = KFold(n_splits=nb_k_splits)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "list_score = []\n",
    "list_auc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train_kf, X_test_kf = X[train_index], X[test_index]\n",
    "    y_train_kf, y_test_kf = y[train_index], y[test_index]\n",
    "    clf.fit(X_train_kf, y_train_kf)\n",
    "    kscore = clf.score(X_test_kf,y_test_kf)\n",
    "    y_pred_kf = clf.predict(X_test_kf)\n",
    "    k_auc = roc_auc_score(y_test_kf, y_pred_kf)\n",
    "    print('fold score: {}, AUC={}'.format(np.round(kscore, decimals=3), np.round(k_auc, decimals=3)))\n",
    "    list_score.append(kscore)\n",
    "    list_auc.append(k_auc)\n",
    "    \n",
    "print('Average score {}-Fold = {}, AUC={}'.format(kf.get_n_splits(X), np.round(np.mean(list_score), decimals=2), np.round(np.mean(list_auc), decimals=2)))\n",
    "\n",
    "# using the training/validate samples, \n",
    "clf.fit(X_train, y_train)\n",
    "score  = clf.score(X_test,y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print('Score training-validation {}, AUC={}'.format(np.round(score , decimals=2), np.round(auc, decimals=2)))\n",
    "\n",
    "print('Score is only valid if classes are balanced, please check AUC instead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_targets = epochs_resampled['Target'].events.shape[0]\n",
    "nb_non_targets = epochs_resampled['NonTarget'].events.shape[0]\n",
    "\n",
    "print('Data contains {}% of Non-targets'.format(np.round(100*(nb_non_targets / (epochs_resampled.events.shape[0])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fad580",
   "metadata": {},
   "source": [
    "### Display train-test LDA classification in a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conf_matrix(y,pred):\n",
    "    cmat = metrics.confusion_matrix(y, pred)\n",
    "    cmat_norm = metrics.confusion_matrix(y, pred, \n",
    "            normalize='true')\n",
    "    ((tn, fp), (fn, tp)) = cmat\n",
    "    ((tnr,fpr),(fnr,tpr)) = cmat_norm\n",
    "    \n",
    "    plt.figure()\n",
    "    labels = ['Non-Target', 'Target']\n",
    "    sns.heatmap(cmat, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xlabel('Ground Truth')\n",
    "    plt.ylabel('Predicted')\n",
    "    \n",
    "    # alternative using sklearn plots\n",
    "    #plt.figure()\n",
    "    #from sklearn.metrics import ConfusionMatrixDisplay\n",
    "    #cm_display = ConfusionMatrixDisplay(cmat).plot()\n",
    "    \n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', \n",
    "                                f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', \n",
    "                                f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "            index=['True 0(Non-Target)', 'True 1(Target)'], \n",
    "            columns=['Pred 0(Non-Target)', \n",
    "                            'Pred 1(Target)'])\n",
    "conf_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba3aed",
   "metadata": {},
   "source": [
    "## Process the ROC curve and precision recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "y_score = clf.decision_function(X_test)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)#.plot()\n",
    "\n",
    "\n",
    "# Precision Recall Display\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_score,\n",
    "                                         pos_label=clf.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall)#.plot()\n",
    "\n",
    "# Display them side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "roc_display.plot(ax=ax1)\n",
    "ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "ax1.legend(loc=\"lower right\")\n",
    "pr_display.plot(ax=ax2)\n",
    "\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_ROC')\n",
    "    fig.savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91533fb",
   "metadata": {},
   "source": [
    "## Signed R-Square plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a2cc9",
   "metadata": {},
   "source": [
    "### Use the function from Wyrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c95b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/bbci/wyrm/blob/master/wyrm/processing.py\n",
    "# Bastian Venthur for wyrm\n",
    "# Code initially from Benjamin Blankertz for bbci (Matlab)\n",
    "\n",
    "def calculate_signed_r_square_mne(epochs, classes=[0,1], classaxis=0, **kwargs):\n",
    "    \"\"\"Calculate the signed r**2 values.\n",
    "    This method calculates the signed r**2 values over the epochs of the\n",
    "    ``dat``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : MNE epoched data\n",
    "    classes: list, optional \n",
    "        (either int index or str for the class name of the epoch))\n",
    "    classaxis : int, optional\n",
    "        the dimension containing epochs\n",
    "    Returns\n",
    "    -------\n",
    "    signed_r_square : ndarray\n",
    "        the signed r**2 values, signed_r_square has one axis less than\n",
    "        the ``dat`` parameter, the ``classaxis`` has been removed\n",
    "    Examples\n",
    "    --------\n",
    "    >>> dat.data.shape\n",
    "    (400, 100, 64)\n",
    "    >>> r = calculate_signed_r_square(dat)\n",
    "    >>> r.shape\n",
    "    (100, 64)\n",
    "    \"\"\"\n",
    "    # TODO: explain the algorithm in the docstring and add a reference\n",
    "    # to a paper.\n",
    "    # select class 0 and 1\n",
    "    # TODO: make class 0, 1 variables\n",
    "    fv1 = epochs[classes[0]]._data\n",
    "    fv2 = epochs[classes[1]]._data\n",
    "    # number of epochs per class\n",
    "    l1 = epochs[classes[0]]._data.shape[classaxis]\n",
    "    l2 = epochs[classes[1]]._data.shape[classaxis]\n",
    "    # calculate r-value (Benjamin approved!)\n",
    "    a = (fv1.mean(axis=classaxis) - fv2.mean(axis=classaxis)) * np.sqrt(l1 * l2)\n",
    "    b = epochs._data.std(axis=classaxis) * (l1 + l2)\n",
    "    r = a / b\n",
    "    # return signed r**2\n",
    "    return np.sign(r) * np.square(r)\n",
    "\n",
    "rsq = calculate_signed_r_square_mne(epochs, classes=['Target','NonTarget'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a0937",
   "metadata": {},
   "source": [
    "### make a pandas database to properly display electrodes and samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pandas database to properly display electrodes and samples\n",
    "fs = epochs.info['sfreq']\n",
    "x = np.float64(list(range(rsq.shape[1])))*(1000/fs)\n",
    "x = x.round(decimals=0).astype(np.int64) + np.int64(pre_epoch*1000)\n",
    "df_rsq = pd.DataFrame(rsq, columns=x, index=epochs.info['ch_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72663ac7",
   "metadata": {},
   "source": [
    "### Plot rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb8ac6",
   "metadata": {},
   "source": [
    "note that using a larger sampling rate will smooth this figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4beac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "hm = sns.heatmap(df_rsq,linewidths=0,cmap=\"coolwarm\").set(title='Signed r-square maps Target vs Non-Target', xlabel='Time (ms)')\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_heatmap' )\n",
    "    plt.savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8339498",
   "metadata": {},
   "source": [
    "### Quickly Display a channel with max rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = None  # <- specify the channel here or it will be selected automatically\n",
    "if picks is None:\n",
    "    ch_max, _ = np.where(rsq == np.max(rsq))\n",
    "    picks = epochs.info['ch_names'][int(ch_max)]\n",
    "\n",
    "#picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "               Target=list(epochs['Target'].iter_evoked()))\n",
    "axs = mne.viz.plot_compare_evokeds(evokeds, picks=picks)  # use combine='mean' if several electrode chosen in picks\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_best_channel')\n",
    "    axs[0].savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdce3c",
   "metadata": {},
   "source": [
    "# Extract accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0834fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
