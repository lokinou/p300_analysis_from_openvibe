{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f07914",
   "metadata": {},
   "source": [
    "# P300 analysis from OpenVibe/BCI2000\n",
    "With lots of cool preprocessing features.\n",
    "source: [https://github.com/lokinou/p300_analysis_from_openvibe](https://github.com/lokinou/p300_analysis_from_openvibe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d777ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the line wit qt below to obtain separate plots\n",
    "%matplotlib inline\n",
    "#%matplotlib qt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d2b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if packages are missing, uncomment and execute here or in anaconda prompt with p300mne env\n",
    "#!pip install \"git+https://github.com/nbara/python-meegkit\"\n",
    "#!pip install statsmodels pyriemann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64932226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import mne\n",
    "import itertools\n",
    "import re\n",
    "from pathlib import Path\n",
    "# LDA\n",
    "from sklearn import metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866800c5",
   "metadata": {},
   "source": [
    "## Define the analysis variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffefc7d",
   "metadata": {},
   "source": [
    "if you don't know how to convert the .ov files, please check my [ov to gdf tutorial](https://github.com/lokinou/openvibe_to_gdf_tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the gdf files\n",
    "#data_dir=r\"C:\\BCI\\dev\\p300_analysis_from_openvibe\"\n",
    "data_dir=r\"./data_sample\"\n",
    "data_type= None  # bci2000 or openvibe or None for autodetection\n",
    "\n",
    "# Define the electrodes here (for the provided sample file)\n",
    "cname = None\n",
    "#cname = ['Fz', 'FC1', 'FC2', 'C1', 'Cz', 'C2', 'P3', 'Pz', 'P4', 'Oz']\n",
    "cname = ['Fz', 'Cz', 'P3', 'Pz', 'P4', 'PO7', 'PO8', 'Oz']\n",
    "\n",
    "# Visual\n",
    "skip_slow_ERP_plot = False  # skip the channelwise ERP plot\n",
    "display_preprocessing_plots = True\n",
    "display_all_erp_plots = True\n",
    "export_figures = True\n",
    "fig_folder = './out'\n",
    "\n",
    "# Preprocessing\n",
    "apply_resample = True # in case the sampling rate is high (>=256Hz)\n",
    "resample_freq = 256 # Hz\n",
    "apply_infinite_reference = False  # rereferencing\n",
    "apply_ASR = True  # use Artifact Subspace Reconstruction (artifact removal)\n",
    "apply_CSD = False  # use Current Source Density (spatial filter)\n",
    "\n",
    "drop_bad_epochs = False\n",
    "reject_channels_full_of_artifacts = True\n",
    "reject_artifactual_epochs = reject_channels_full_of_artifacts and True # do not reject epochs if you dont reject channels or use CSD\n",
    "artifact_threshold = 100e-6\n",
    "ratio_tolerated_artifacts = 0.3  # if 30% of artifacts in 200ms windows, then the channel is rejected\n",
    "\n",
    "# ERP analysis parameters (values in sec)\n",
    "pre_epoch = -.2\n",
    "epoch_length = .6\n",
    "baseline = (-.2, 0)\n",
    "#isi = .0625\n",
    "#flash = .125\n",
    "\n",
    "\n",
    "# LDA\n",
    "resample_LDA = 32 # Hz\n",
    "nb_k_splits = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ad521-f8ab-4575-8048-e2f6ce9263eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For internal processing, stimuli begin at 100 to discriminate from MNE usually using stimuli 1 and 0 as target and non-target \n",
    "stimulus_padding  = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041485e",
   "metadata": {},
   "source": [
    "## Load the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eda1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BCI2kReader import BCI2kReader as b2k\n",
    "from BCI2kReader import FileReader as f2k\n",
    "\n",
    "raws = []\n",
    "\n",
    "def extract_annotations(filename, verbose=False):\n",
    "    display_preprocessing_plots = False\n",
    "    file= b2k.BCI2kReader(filename)\n",
    "\n",
    "    if verbose:\n",
    "        print(file.states)\n",
    "    target_states = np.squeeze(file.states['StimulusType'])\n",
    "    stimulus_codes = np.squeeze(file.states['StimulusCode'])\n",
    "    if 'StimulusBegin' in file.states.keys():\n",
    "        stimulus_begin = np.squeeze(file.states['StimulusBegin'])\n",
    "    else:\n",
    "        stimulus_begin = np.squeeze(file.states['Flashing'])\n",
    "\n",
    "    phase = np.squeeze(file.states['PhaseInSequence'])\n",
    "\n",
    "    fs = file.samplingrate\n",
    "\n",
    "    idx_targets = np.where(target_states)[0]\n",
    "    idx_codes = np.where(stimulus_codes>0)[0]\n",
    "    idx_begin = np.where(stimulus_begin>0)[0]\n",
    "\n",
    "\n",
    "    # In BCI2000 states are maintained over different samples, we search here the differences of when the codes are > 0\n",
    "    groups = np.split(idx_codes, np.where(np.diff(idx_codes) != 1)[0]+1)\n",
    "    # we take the first sample where a difference can be found\n",
    "    code_change_idx = np.array([g[0] for g in groups])\n",
    "    #[idx_codes[idx] for idx in code_change_idx]\n",
    "    print('nb stimuli={}'.format(len(code_change_idx)))\n",
    "\n",
    "    # we intersect the target index list with the code change to find the onset of targets and non-targets\n",
    "    target_idx=np.intersect1d(code_change_idx,idx_targets)\n",
    "    print('nb targets={}'.format(len(target_idx)))\n",
    "    non_target_idx= np.setdiff1d(code_change_idx,idx_targets)\n",
    "\n",
    "    # Translating into MNE Annotations \n",
    "    # define the annotations from the recovered stimuli (in seconds)\n",
    "    sample_lengh = 1/fs\n",
    "    onsets = code_change_idx * sample_lengh\n",
    "    onsets = np.repeat(onsets, 2)  # repeat onsets\n",
    "    # define the descriptio\n",
    "    description_targets = np.zeros(code_change_idx.shape, dtype=np.uint)\n",
    "    # index of targets in the list of stimuli onsets\n",
    "    description_targets[np.searchsorted(code_change_idx, target_idx)] = 1\n",
    "    description_codes = stimulus_codes[code_change_idx] + stimulus_padding  # start codes at 100 because 0 and 1 are used for target and nontarget\n",
    "    # merge code and target decriptions\n",
    "    description = np.zeros(description_targets.shape[0]*2, dtype=np.uint)\n",
    "    description[np.arange(description_targets.shape[0]*2, step=2)] = description_codes\n",
    "    description[np.arange(start=1, stop=(description_targets.shape[0]*2)+1, step=2)] = description_targets\n",
    "\n",
    "    if display_preprocessing_plots:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(description[:100])\n",
    "        fig.suptitle('Targets(1) and non-targets(0) for 100 first stimuli')\n",
    "\n",
    "    if display_preprocessing_plots:\n",
    "        fig = plt.figure()\n",
    "        plt.plot(phase == 1)\n",
    "        fig.suptitle('Trial begin')\n",
    "\n",
    "    # extract trial begin markers  #  this method does not work since some stimuli are declared before phase==1\n",
    "    # let's think baclwards use the end markers instead\n",
    "    new_phase_continuous = np.where(phase == 1)[0]\n",
    "    groups = np.split(new_phase_continuous, np.where(np.diff(new_phase_continuous) != 1)[0]+1)\n",
    "    new_trial_idx = np.array([g[0] for g in groups])\n",
    "    \n",
    "    \n",
    "    # extract trial end markers\n",
    "    new_phase_continuous = np.where(phase == 3)[0]\n",
    "    groups = np.split(new_phase_continuous, np.where(np.diff(new_phase_continuous) != 1)[0]+1)\n",
    "    end_of_trial_idx = np.array([g[-1] for g in groups]) # take the last index to integrate all post sequence duration\n",
    "    \n",
    "    # deduce trial begin markers  # \n",
    "    #new_trial_idx = np.zeros(end_of_trial_idx.size)\n",
    "    #new_trial_idx[1:] = end_of_trial_idx[1:]+1\n",
    "\n",
    "    if new_trial_idx.shape[0] > end_of_trial_idx[0]:\n",
    "        print('WARNING: no end of trial for the last trial (interrupted recording?), it will be ignored for offline accuracy calculation')\n",
    "        inter_trial_duration = end_of_trial_idx[0:len(new_trial_idx)] - new_trial_idx\n",
    "    else:\n",
    "        inter_trial_duration = end_of_trial_idx - new_trial_idx\n",
    "    inter_trial_duration = inter_trial_duration * sample_lengh  # express in seconds\n",
    "\n",
    "\n",
    "    print(\"Extracted {} trials\".format(len(new_trial_idx)))\n",
    "\n",
    "    # set a non-zero duration for stimuli (or MNE ignores them)\n",
    "    duration = np.ones(onsets.shape) * sample_lengh\n",
    "\n",
    "\n",
    "    # merge phase in sequence events with stimuli onsets\n",
    "    onsets_phase = new_trial_idx * sample_lengh\n",
    "    onsets = np.concatenate((onsets, onsets_phase))\n",
    "    \n",
    "    duration = np.concatenate((duration, inter_trial_duration))\n",
    "    description = np.concatenate((description, np.ones(new_trial_idx.shape) * 10))  # concatenate trials markers=10\n",
    "    srt = np.argsort(onsets) # sort according to their timing\n",
    "    onsets=onsets[srt]\n",
    "    duration = duration[srt]\n",
    "    description = description[srt].astype(np.uint8)\n",
    "    inter_trial_duration\n",
    "    annotations = mne.Annotations(onset=onsets, duration=duration, description=description)\n",
    "\n",
    "    file.flush()\n",
    "    return annotations\n",
    "\n",
    "def load_bci2k(filename_list, verbose=False):\n",
    "    \"\"\"\n",
    "    return MNE raw, number of rows in the matrix\n",
    "    \"\"\"\n",
    "    raws = []\n",
    "    for fn in filename_list:\n",
    "        cname = None\n",
    "        with b2k.BCI2kReader(fn) as file:\n",
    "            \n",
    "            # Extract signals and states\n",
    "            print('opened')\n",
    "            eeg_data = file.signals\n",
    "            states = file.states\n",
    "            fs = file.samplingrate\n",
    "            nb_chan = eeg_data.shape[0]\n",
    "            #file.purge()\n",
    "\n",
    "            # Extract channel names\n",
    "            reader = f2k.bcistream(fn)\n",
    "            if verbose:\n",
    "                print(reader.params)\n",
    "            # actualize the parameters by including the defined channel names\n",
    "            if len(reader.params['ChannelNames']):\n",
    "                if cname != reader.params['ChannelNames']:\n",
    "                    cname = reader.params['ChannelNames']\n",
    "                    print('Actualized channel names to {}'.format(cname))\n",
    "\n",
    "            if cname is None:\n",
    "                cname = [str(ch_n) for ch_n in list(range(nb_chan))]\n",
    "                \n",
    "            # extract the number of rows\n",
    "            nb_stim_rows = np.uint8(reader.params['NumMatrixRows'][0])\n",
    "            nb_stim_cols = np.uint8(reader.params['NumMatrixColumns'][0])\n",
    "            nb_seq = np.uint8(reader.params['NumberOfSequences'])\n",
    "\n",
    "            # convert states into annotations\n",
    "            info = mne.create_info(cname, fs, ch_types='eeg', verbose=None)\n",
    "            raw_array = mne.io.RawArray(eeg_data, info)\n",
    "            # Manually force the filename or mne complains\n",
    "            raw_array._filenames = [os.path.basename(fn)]\n",
    "            \n",
    "            annotations = extract_annotations(fn, verbose= False)\n",
    "            raw_array.set_annotations(annotations)\n",
    "            raws.append(raw_array)\n",
    "    return raws, (nb_stim_rows, nb_stim_cols, nb_seq)\n",
    "\n",
    "#fn = [\"./data_sample/bci2000\\Heide_einsteinBP_calibration4S001R01.dat\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a34fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_stimlus_rows = None  # stores the number of rows in the P300 to separate rows and columns\n",
    "os.path.exists(data_dir)\n",
    "fnames = []\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".gdf\"):\n",
    "        data_type = 'openvibe'\n",
    "        fnames.append(os.path.join(data_dir, file))\n",
    "        print(os.path.join(data_dir, file))\n",
    "    elif file.endswith(\".dat\"):\n",
    "        data_type = 'bci2000'\n",
    "        print(os.path.join(data_dir, file))\n",
    "        fnames.append(os.path.join(data_dir, file))\n",
    "print(fnames)\n",
    "if data_type == 'openvibe':\n",
    "    # load and preprocess data ####################################################\n",
    "    raws = [mne.io.read_raw_gdf(f, preload=True) for f in fnames]\n",
    "elif data_type == 'bci2000':\n",
    "    raws, (nb_stimlus_rows, nb_stimulus_cols, nb_seq) = load_bci2k(fnames, verbose=True)\n",
    "raw = mne.concatenate_raws(raws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29780a12-ad87-46f7-b00a-8992734229e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_stimulus_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebeb81d-d08c-4a59-a013-fe1ecb6dda0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f9a3b-9862-47d9-9eb4-f3e618e76379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58513759-e223-4bd4-8f6d-9da09a7b1e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471aa253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the variance of the data is >1, it means the data is expressed in microvolts\n",
    "# Since MNE uses Volt as a default value, we rescale microvolts to volt\n",
    "if np.var(raw._data)>1:\n",
    "    raw._data = raw._data * 1.e-6\n",
    "    print('Rescaled signal to Volt (mean variance={})'.format(np.var(raw._data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4518983d",
   "metadata": {},
   "source": [
    "Create a name for figures output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = Path(raw._filenames[0]).stem\n",
    "if len(raw._filenames)>1:\n",
    "    output_name = output_name + '_{}_files'.format(len(raw._filenames))\n",
    "print('Figures will have the name: {}'.format(output_name))\n",
    "\n",
    "fig_folder = os.path.join(fig_folder,output_name)\n",
    "if not os.path.exists(fig_folder):\n",
    "    os.mkdir(fig_folder)\n",
    "    print('Creating output directory'.format(fig_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4461997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_preprocessing_plots:\n",
    "    raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97b8f2",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b534c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_resample:\n",
    "    raw.resample(resample_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60242a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "montage = mne.channels.make_standard_montage('standard_1005')\n",
    "\n",
    "if cname is None:\n",
    "    cname = raw.info['ch_names']\n",
    "    print('Using channel names directly from the data files: {}'.format(cname))\n",
    "else:\n",
    "    print('Using user defined channel names: {}'.format(cname))\n",
    "        \n",
    "\n",
    "nb_chan = len(raw.info['ch_names'])\n",
    "nb_def_ch = len(cname)\n",
    "\n",
    "# regular expressions to look for certain channel types\n",
    "eog_pattern = re.compile('eog|EOG')  # EOG electrodes\n",
    "emg_pattern = re.compile('emg|EMG')  # EMG electrodes\n",
    "mastoid_pattern = re.compile('^[aA][0-9]+')  # A1 and A2 electrodes (mastoids)\n",
    "\n",
    "\n",
    "types = []\n",
    "cname_map = dict(zip(raw.info['ch_names'], cname))\n",
    "for nc in cname:\n",
    "    if eog_pattern.search(nc) is not None:\n",
    "        t = 'eog'\n",
    "    elif emg_pattern.search(nc) is not None:\n",
    "        t = 'emg'\n",
    "    elif mastoid_pattern.search(nc) is not None:\n",
    "        t = 'misc'\n",
    "    elif nc in montage.ch_names:  # check the 10-05 montage for all electrode names\n",
    "        t = 'eeg'\n",
    "    else:\n",
    "        t = 'misc'  # if not found, discard the channel as misc\n",
    "    \n",
    "    types.append(t)\n",
    "        \n",
    "type_map = dict(zip(cname, types))\n",
    "\n",
    "# rename and pick eeg\n",
    "raw.rename_channels(cname_map, allow_duplicates=False)\n",
    "raw.set_channel_types(type_map)\n",
    "raw.pick_types(eeg=True, misc=False)\n",
    "print('Electrode mapping')\n",
    "type_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475591f",
   "metadata": {},
   "source": [
    "rename the electrodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bdc3cc",
   "metadata": {},
   "source": [
    "Set the montage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46773eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.set_montage(montage, match_case=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199c75a",
   "metadata": {},
   "source": [
    "Check whether there are bad channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef84aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 50Hz power variance\n",
    "psd, freqs = mne.time_frequency.psd_welch(raw,verbose=True)\n",
    "power_50hz = psd[:,np.where(freqs ==60)]\n",
    "print('50Hz variance: {}'.format(mne.preprocessing.bads._find_outliers(power_50hz.squeeze(), threshold=3, max_iter=5, tail=0)))\n",
    "\n",
    "\n",
    "# using variance\n",
    "ch_var  = [np.var(raw._data[i,:]) for i in list(range(raw._data.shape[0]))]\n",
    "print('Variance: {}'.format(mne.preprocessing.bads._find_outliers(ch_var, threshold=3, max_iter=5, tail=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e9f7e",
   "metadata": {},
   "source": [
    "rereferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932dc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_infinite_reference:\n",
    "    raw.del_proj()  # remove our average reference projector first\n",
    "    sphere = mne.make_sphere_model('auto', 'auto', raw.info)\n",
    "    src = mne.setup_volume_source_space(sphere=sphere, exclude=30., pos=15.)\n",
    "    forward = mne.make_forward_solution(raw.info, trans=None, src=src, bem=sphere)\n",
    "    raw_rest = raw.copy().set_eeg_reference('REST', forward=forward)\n",
    "    \n",
    "    if display_preprocessing_plots:\n",
    "        for title, _raw in zip(['Original', 'REST (∞)'], [raw, raw_rest]):\n",
    "            fig = _raw.plot(n_channels=len(raw), scalings=dict(eeg=5e-5))\n",
    "            # make room for title\n",
    "            fig.subplots_adjust(top=0.9)\n",
    "            fig.suptitle('{} reference'.format(title), size='xx-large', weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ce84f",
   "metadata": {},
   "source": [
    "## Bandpass the signal\n",
    "Removes noise and drift from the EEG signal by applying a infinite impulse response (two-pass) filter between .5 and 40Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb0b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.filter(.5, 40, fir_window='hann', method='iir')\n",
    "raw.notch_filter(50)  # removes 50Hz noise\n",
    "if display_preprocessing_plots:\n",
    "    raw.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224b322",
   "metadata": {},
   "source": [
    "## Excluding of channels full of artifacts (muscular or disconnecting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "#Apply a variance based channel rejection if artifacts are present >30% of the time\n",
    "def detec_rej_channel(raw, duration=.2, overlap_duration=.1, threshold_eeg=artifact_threshold, reject_ratio=ratio_tolerated_artifacts):\n",
    "\n",
    "    epochs_rej = mne.make_fixed_length_epochs(raw,duration=duration, overlap=overlap_duration, preload=True)\n",
    "    epochs_rej._data.shape\n",
    "    diff = np.max(epochs_rej._data, axis=2) - np.min(epochs_rej._data, axis=2)\n",
    "\n",
    "    print(diff.shape)\n",
    "\n",
    "    rej = (diff>=threshold_eeg).astype(np.float64)\n",
    "    rel = sns.heatmap(rej)\n",
    "    rel.set(title='Detected artifacts per electrode and runs (white)')\n",
    "\n",
    "    # calculate ratio of rejected trials\n",
    "    ratios = np.sum(rej,axis=0) / rej.shape[0]\n",
    "    \n",
    "    ret = np.argwhere(ratios >= reject_ratio).tolist()\n",
    "    if len(ret)>0 and len(ret[0]):\n",
    "        print('Found {} channels with at least {}% {}s epochs > {} amplitude)'.format(len(ret), \n",
    "                                                                                              reject_ratio*100, duration,\n",
    "                                                                                      threshold_eeg))\n",
    "        return ret[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "if reject_channels_full_of_artifacts:\n",
    "    rej_ch = detec_rej_channel(raw)\n",
    "    if rej_ch is not None:\n",
    "        new_bads = [raw.info['ch_names'][ch] for ch in rej_ch]\n",
    "        raw.info['bads'].extend(new_bads)\n",
    "        raw.pick_types(eeg=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4563dccf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Artifact Subspace Reconstruction fitting and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5a650",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_ASR:\n",
    "    #!pip install meegkit pymanopt\n",
    "    from meegkit.asr import ASR\n",
    "    fs = int(raw.info[\"sfreq\"])  # sampling frequency\n",
    "    method='riemann'  # if error, use 'euclid'\n",
    "    method='euclid' # pymanopt library still buggy\n",
    "    window_s=.5  # .5 sec window of analysis\n",
    "    data_interval_s  = None # (begin, end) in sec of the training sample\n",
    "    estimator='lwf'  #leave blank if using euclidian mode \n",
    "\n",
    "    # define the ASR model using riemannian method\n",
    "    #asr_model = ASR(sfreq=fs, method=method, win_len=window_s, estimator=estimator)\n",
    "\n",
    "    # if failing (after trying twice. SVD error occurs for no reason sometimes)\n",
    "    asr_model = ASR(sfreq=fs, method=method, win_len=window_s)\n",
    "\n",
    "    # The best would be to choose another recording during the same session to train the model without overfitting\n",
    "    data = raw._data  # the numpy array with data is stored in the _data variable\n",
    "\n",
    "    # Select a time interval for training data\n",
    "    train_idx = None\n",
    "    if data_interval_s is not None:\n",
    "        train_idx = np.arange(data_interval_s[0] * fs, data_interval_s[1] * fs, dtype=int)\n",
    "    # otherwise select the whole training set\n",
    "    else:\n",
    "        train_idx = np.arange(0, data.shape[1])\n",
    "\n",
    "    train_data = data[:, train_idx]\n",
    "    print('Training on samples of size {}'.format(train_data.shape))\n",
    "\n",
    "    # fir the ASR model with data intervals\n",
    "    _, sample_mask = asr_model.fit(train_data)\n",
    "    print('Model trained')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a62943a",
   "metadata": {},
   "source": [
    "### Clean the current dataset\n",
    "Please check whether using this artifact filtering method increases signal to noise ratio rather than reducing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if apply_ASR:\n",
    "    clean =  asr_model.transform(raw._data)\n",
    "\n",
    "    display_window_s = 60  # \n",
    "\n",
    "    if display_preprocessing_plots:  #\n",
    "        data_p = raw._data[0:fs*display_window_s]  # reshape to (n_chans, n_times)\n",
    "        clean_p = clean[0:fs*display_window_s]\n",
    "\n",
    "        ###############################################################################\n",
    "        # Plot the results\n",
    "        # -----------------------------------------------------------------------------\n",
    "        #\n",
    "        # Data was trained on a 40s window from 5s to 45s onwards (gray filled area).\n",
    "        # The algorithm then removes portions of this data with high amplitude\n",
    "        # artifacts before running the calibration (hatched area = good).\n",
    "        nb_ch_disp = len(raw.info['ch_names'])\n",
    "        times = np.arange(data_p.shape[-1]) / fs\n",
    "        f, ax = plt.subplots(nb_ch_disp, sharex=True, figsize=(32, 16))\n",
    "        for i in range(nb_ch_disp):\n",
    "            # ax[i].fill_between(train_idx / fs, 0, 1, color='grey', alpha=.3,\n",
    "            #                   transform=ax[i].get_xaxis_transform(),\n",
    "            #                   label='calibration window')\n",
    "            # ax[i].fill_between(train_idx / fs, 0, 1, where=sample_mask.flat,\n",
    "            #                   transform=ax[i].get_xaxis_transform(),\n",
    "            #                   facecolor='none', hatch='...', edgecolor='k',\n",
    "            #                   label='selected window')\n",
    "            ax[i].plot(times, data_p[i], lw=.5, label='before ASR')\n",
    "            ax[i].plot(times, clean_p[i], label='after ASR', lw=.5)\n",
    "            # ax[i].plot(times, raw[i]-clean[i], label='Diff', lw=.5)\n",
    "            # ax[i].set_ylim([-50, 50])\n",
    "            ax[i].set_ylabel(f'ch{i}')\n",
    "            ax[i].set_yticks([])\n",
    "        ax[i].set_xlabel('Time (s)')\n",
    "        ax[0].legend(fontsize='small', bbox_to_anchor=(1.04, 1), borderaxespad=0)\n",
    "        plt.subplots_adjust(hspace=0, right=0.75)\n",
    "        plt.suptitle('Before/after ASR')\n",
    "        plt.show()\n",
    "    raw.data_ = clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a02ab2",
   "metadata": {},
   "source": [
    "### Convert text annotations (i.e. unprocessed events) into events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e61c1cd",
   "metadata": {},
   "source": [
    "**Small but major hack to realign events due to conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type=='openvibe':\n",
    "    print(\"Erroneous annotations: {}\".format(raw.annotations.description))\n",
    "    print('Note here that the first annotation is 0 or 1, this is an error and thus we shift the annotations to retrieve the correct timings')\n",
    "    raw.annotations.description = np.roll(raw.annotations.description, -1)\n",
    "    print(\"Corrected annotations: {}\".format(raw.annotations.description))\n",
    "\n",
    "    # in case you want to debug the issue, I left here a way to visualize them\n",
    "    # retrieving the list of annotations\n",
    "    import pprint\n",
    "    print(raw.annotations.to_data_frame())\n",
    "    df = raw.annotations.to_data_frame()\n",
    "    print('Displaying all annotations')\n",
    "    annot_codes = [np.int64(n) for n in np.unique(df['description'])]\n",
    "    df['description'] = df['description'].astype(int)\n",
    "\n",
    "    if False:\n",
    "        # to see and debug the fill list of annotations\n",
    "        import pandas as pd\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        #a = df[df['description'] != 33286]\n",
    "        #print(a)\n",
    "        print(df)\n",
    "        pd.set_option('display.max_rows', 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd225eb",
   "metadata": {},
   "source": [
    "### Make a list of the annotations to check whether all stimuli can be found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f06080",
   "metadata": {
    "tags": []
   },
   "source": [
    "These annotations seem to relate to hex codes. OpenViBE definitions can be found on [OpenViBE's website](http://openvibe.inria.fr/stimulation-codes/). Let's parse the copypasted list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cc165b",
   "metadata": {},
   "source": [
    "Make a dataframe of the stimuli in common between both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d00367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "if data_type=='openvibe':\n",
    "    tr_sim= ''\n",
    "    pat_extract= re.compile('^([^ ]+)[ ]+0x[0-9A-Fa-f]+[ \\/]+([0-9]+)')\n",
    "    #OVTK_GDF_125_Watt                                     0x585       //  1413\n",
    "    k_stim = []\n",
    "    k_stim_int = []\n",
    "    v_stim = []\n",
    "\n",
    "    # read and convert annotations\n",
    "    with open(r'.\\ov_stims.txt', 'r') as fd:\n",
    "        for line in fd.readlines():\n",
    "            m = pat_extract.match(line)\n",
    "            v, k = m.groups()\n",
    "            k_stim.append(k)\n",
    "            k_stim_int.append(int(k))\n",
    "            v_stim.append(v)\n",
    "\n",
    "    # format dict and list\n",
    "    stim_map = dict(zip(k_stim_int, v_stim))\n",
    "    stim_map_inv = dict(zip(v_stim, k_stim))\n",
    "\n",
    "    stim_tup = list(zip(k_stim_int, v_stim))\n",
    "\n",
    "    df = pd.DataFrame.from_dict(stim_tup)\n",
    "    df.columns = ['coden', 'desc']\n",
    "    df[[c in annot_codes for c in df.coden]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bfa34d",
   "metadata": {},
   "source": [
    "From this table, we could locate and save the codes for **Target and Non-Target** and give them the following values: target=1 and non-target=0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb026c75-d5e3-4cab-9123-549923b876cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8fbd3-9c5e-453b-bb5c-10c6b4b51b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out string annotations\n",
    "string_annotations = np.where(~np.apply_along_axis(np.char.isnumeric,0,raw.annotations.description))[0]\n",
    "if string_annotations.size > 0:\n",
    "    print('removing all string annotations: {}'.format(raw.annotations.description[string_annotations]))\n",
    "    raw.annotations.delete(string_annotations)\n",
    "# Extractz row and column labels from stimuli numbers\n",
    "stimli_labels = np.sort(np.unique(raw.annotations.description[np.where(raw.annotations.description.astype(np.uint8) >= stimulus_padding)]))\n",
    "\n",
    "row_labels = stimli_labels[:nb_stimlus_rows]\n",
    "col_labels = stimli_labels[nb_stimlus_rows:nb_stimlus_rows+nb_stimulus_cols]\n",
    "\n",
    "print(\"Row labels: {}\".format(row_labels))\n",
    "print(\"Col labels: {}\".format(col_labels))\n",
    "\n",
    "# map the stimuli for MNE to read\n",
    "map_stimuli = dict(zip(stimli_labels, stimli_labels.astype(np.uint8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686637bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = 1, nontarget = 0\n",
    "target_map = None\n",
    "if data_type == 'openvibe':\n",
    "    target_map = {'33286':0, '33285':1}\n",
    "elif data_type == \"bci2000\":\n",
    "    target_map = {'0':0, '1':1, '10':10}\n",
    "    target_map.update(map_stimuli)\n",
    "target_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38d6e4e",
   "metadata": {},
   "source": [
    "Then we can convert annotations into events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24830d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events, event_id = mne.events_from_annotations(raw, event_id=target_map)\n",
    "print(\"Found {} events\".format(len(all_events[:])))\n",
    "event_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d024a2da",
   "metadata": {},
   "source": [
    "### Pick the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9983d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick all channels\n",
    "picks = mne.pick_channels(raw.info[\"ch_names\"], include=[])\n",
    "picks\n",
    "raw.plot_sensors(show_names=True)\n",
    "fig = raw.plot_sensors('3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc1d00",
   "metadata": {},
   "source": [
    "## Epoching from events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65f7ee",
   "metadata": {},
   "source": [
    "Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa1ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "all_events[:, 0]\n",
    "a = np.array(all_events[:, 0])\n",
    "dups = [item for item, count in Counter(a).items() if count > 1]\n",
    "if dups:\n",
    "    print(\"WARNING: Duplicate found at sample(s) {}\".format(dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf5109-f749-4e39-8a70-e5a340723373",
   "metadata": {},
   "source": [
    "### Prepare metadata to annotate events\n",
    "The goal here is to annotate every event with the Trial, stimulus and isRow information for offline accuracy calculations.\\\n",
    "It takes the shape of a dataframe with the number of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8aded-e4dd-4013-ad9b-e062061f4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the empty dataframe\n",
    "nb_events = np.count_nonzero(np.where(all_events[:,2] <= 1))\n",
    "df_meta = pd.DataFrame(data=None, index=list(range(0,nb_events)), columns=['Trial_nb', 'stim', 'is_row', 'is_col', 'is_target'])\n",
    "ev = all_events.copy()\n",
    "# populate with trial information\n",
    "g_trial = np.split(all_events[:,2], np.where(all_events[:,2]==10)[0])[1:]\n",
    "\n",
    "\n",
    "# iterate over trials to extract their metadata\n",
    "cur_trial_n = 0\n",
    "cursor=0\n",
    "for g in g_trial:\n",
    "    cur_trial_n += 1\n",
    "    #print('Trial {}'.format(cur_trial_n))\n",
    "    idx_stim_labs = np.where(g >= stimulus_padding)  \n",
    "    stim_labels = g[idx_stim_labs]  # extract stimulus labels\n",
    "    # isolate target and non_target stimuli\n",
    "    targets_nontargets_phase = np.delete(g.copy(),idx_stim_labs)\n",
    "    targets_nontargets = np.delete(targets_nontargets_phase,np.where(targets_nontargets_phase == 10))\n",
    "    # extract the stimulus row/column information\n",
    "    is_row = stim_labels <= nb_stimlus_rows+stimulus_padding\n",
    "    \n",
    "    # deduct the cursor over stimulus event\n",
    "    end_cursor = cursor + len(targets_nontargets)\n",
    "    \n",
    "    # build up the metadata dataframe\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'Trial_nb'] = cur_trial_n\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'stim'] = stim_labels\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'is_row'] = is_row.astype(np.uint8)\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'is_col'] = np.invert(is_row).astype(np.uint8)\n",
    "    df_meta.loc[list(range(cursor, end_cursor)),'is_target'] = targets_nontargets.astype(np.uint8)\n",
    "    \n",
    "    cursor = end_cursor\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb307d91-3ead-4e7b-b845-406dd61bc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(all_events[:,2].astype(np.uint8) <= 1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb0fb5-f827-4b14-884e-6170beee1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_events[:,2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fbe57-4369-43a5-94a3-fa111639cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we use metadata we can pick only target and non-target events\n",
    "events = mne.pick_events(all_events, [0,1])\n",
    "\n",
    "event_ids = dict(NonTarget=0, Target=1) \n",
    "#isi = isi\n",
    "#flash = flash\n",
    "pre_epoch = pre_epoch\n",
    "epoch_length = epoch_length\n",
    "\n",
    "\n",
    "# epoching function\n",
    "epochs = mne.Epochs(raw, events, baseline=baseline, event_id=event_ids, tmin=pre_epoch, tmax=epoch_length, event_repeated='drop', picks = ['eeg', 'csd'],\n",
    "                    preload=True, metadata=df_meta)\n",
    "\n",
    "# if there is any delay,\n",
    "#epochs.shift_time(-isi, relative=True)\n",
    "if display_preprocessing_plots:\n",
    "    fig = epochs.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2898ef39-b918-4c62-9810-4697162934b4",
   "metadata": {},
   "source": [
    "using the metadata, we can now how many events are in our first epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2a1e1c-a32d-46f7-83e3-329e78e35299",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs[epochs.metadata['Trial_nb'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a91f1a",
   "metadata": {},
   "source": [
    "### Making a cross correlation plot between the electrodes to see how channels relate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3ff9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.corrcoef(raw._data)\n",
    "fig = plt.figure()\n",
    "hm = sns.heatmap(m,linewidths=0,cmap=\"YlGnBu\").set(title='Cross correlation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced88097",
   "metadata": {},
   "source": [
    "### Epoch rejection\n",
    "Please filter out channels before epochs. A problematic channel can discard the whole recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c142c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if reject_artifactual_epochs:\n",
    "    reject_criteria = dict(eeg=150e-6)  # 100 µV  #eog=200e-6)\n",
    "    _ = epochs.drop_bad(reject=reject_criteria)\n",
    "    if display_preprocessing_plots:\n",
    "        epochs.plot_drop_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d30c19a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apply current source density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3abf42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if apply_CSD:\n",
    "    epochs_csd = mne.preprocessing.compute_current_source_density(epochs)\n",
    "    epochs = epochs_csd\n",
    "    if display_preprocessing_plots:\n",
    "        epochs_csd.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46a29b",
   "metadata": {},
   "source": [
    "### Average the epochs of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68106d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_nt = epochs['NonTarget'].average()\n",
    "l_target = epochs['Target'].average()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6729d3",
   "metadata": {},
   "source": [
    "target and non target signal plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    fig1 = l_target.plot(spatial_colors=True, axes=ax[0], show=False)\n",
    "    fig2 = l_nt.plot(spatial_colors=True, axes=ax[1], show=False)\n",
    "    # Add title\n",
    "    fig.suptitle(\"Target(top) - Non-Target(bottom)\")\n",
    "    # Fix font spacing\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57d133",
   "metadata": {},
   "source": [
    "target and non target signal topomaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf7661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    spec_kw = dict(width_ratios=[1,1,1,.15], wspace=0.5,\n",
    "                   hspace=0.5,height_ratios=[1,1])\n",
    "                             #hspace=0.5, height_ratios=[1, 2])\n",
    "\n",
    "    fig, ax = plt.subplots(2, 4, gridspec_kw=spec_kw)\n",
    "    l_target.plot_topomap(times=[0, 0.18, 0.4], average=0.05, axes=ax[0,:], show=False)\n",
    "    l_nt.plot_topomap(times=[0, 0.18, 0.4], average=0.05, axes=ax[1,:], show=False)\n",
    "    fig.suptitle(\"Target(top) - Non-Target(bottom)\")\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a88e9",
   "metadata": {},
   "source": [
    "joint plot (of the two former graphs). Plase not that Y scales differ between plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05494451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l_target.plot_joint()\n",
    "plt.gcf().canvas.set_window_title('Target joint plot')\n",
    "l_nt.plot_joint()\n",
    "plt.gcf().canvas.set_window_title('Non-Target joint plot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1c6bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Average plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f086da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "                   Target=list(epochs['Target'].iter_evoked()))\n",
    "    #picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "    mne.viz.plot_compare_evokeds(evokeds, picks=picks, combine='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065fb9b1",
   "metadata": {},
   "source": [
    "### Target vs NonTarget Erps per channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ddccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_slow_ERP_plot:\n",
    "    nb_chans = epochs['Target']._data.shape[1]\n",
    "    splt_width = int(np.floor(np.sqrt(1.0*nb_chans+1)))  # adding an extra plot with all channels combined at the end\n",
    "    splt_height = splt_width if splt_width * splt_width >= nb_chans+1 else splt_width+1\n",
    "    if splt_height * splt_width < nb_chans+1:\n",
    "        splt_height += 1\n",
    "    fig, ax = plt.subplots(splt_height,splt_width)\n",
    "\n",
    "    evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "                   Target=list(epochs['Target'].iter_evoked()))\n",
    "    #picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "\n",
    "    shape_epochs = epochs['Target']._data.shape\n",
    "    for ch_idx in range(nb_chans):\n",
    "        print('plotting channel {}'.format(ch_idx+1))\n",
    "        mne.viz.plot_compare_evokeds(evokeds,picks=[epochs.info['ch_names'][ch_idx]],\n",
    "                                     legend=False,\n",
    "                                     axes=ax[ch_idx//splt_width, ch_idx%splt_width], show=False)\n",
    "        #plt.show(block=False)\n",
    "        plt.subplots_adjust(hspace=0.5, wspace=.5)\n",
    "        #plt.pause(.1)\n",
    "    print('plotting averaged channels')\n",
    "    axs = mne.viz.plot_compare_evokeds(evokeds, picks=picks, combine='mean',\n",
    "                                 legend=True,\n",
    "                                 axes=ax[-1,-1], show=False)\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=.5)\n",
    "    plt.show()\n",
    "    print(\"Please note that this plot is optimized for higher resolution and has the legend overlapping the average\")\n",
    "\n",
    "    if export_figures:\n",
    "        out_name = os.path.join(fig_folder, output_name + '_ERPs')\n",
    "        axs[0].savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7930da8",
   "metadata": {},
   "source": [
    "### Display epoch at Cz and Pz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fdec8",
   "metadata": {},
   "source": [
    "### Display single epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd310e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if display_all_erp_plots:\n",
    "    epochs['Target'].plot_image(combine='mean')\n",
    "    plt.gcf().canvas.set_window_title('Target')\n",
    "    epochs['NonTarget'].plot_image(combine='mean')\n",
    "    plt.gcf().canvas.set_window_title('Non-Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c0e3b",
   "metadata": {},
   "source": [
    "### Same plot but channel wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce1db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_slow_ERP_plot:\n",
    "    dict_electrodes = dict(eeg='EEG') if not apply_CSD else dict(csd='CSD')\n",
    "    if display_all_erp_plots:\n",
    "        for ch_type, title in dict_electrodes.items():\n",
    "            layout = mne.channels.find_layout(epochs.info, ch_type=ch_type)\n",
    "            epochs['Target'].plot_topo_image(layout=layout, fig_facecolor='w',\n",
    "                                                    font_color='k', title=title+' Target Trial x time amplitude')\n",
    "            epochs['NonTarget'].plot_topo_image(layout=layout, fig_facecolor='w',\n",
    "                                                    font_color='k', title=title+' Non-Target Trial x time amplitude')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4d9f1",
   "metadata": {},
   "source": [
    "# Assess single epoch classification accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5057efa",
   "metadata": {},
   "source": [
    "resample the signal, we don't need that much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a299028",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fs = resample_LDA #\n",
    "epochs_resampled = epochs.copy().resample(new_fs)\n",
    "print('resampling to {}Hz'.format(new_fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2f656",
   "metadata": {},
   "source": [
    "modify the data matrix to be properly assessed via LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f99f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X = epochs_resampled._data[:,1,:]  # input data at CZ (TODO:flatten all electrodes)\n",
    "X = epochs_resampled._data[:,:,:]  # input data at CZ (TODO:flatten all electrodes)\n",
    "y = epochs_resampled.events[:,2]  # ground truth\n",
    "\n",
    "# remove the information \n",
    "\n",
    "def reshape_mne_raw_for_lda(X, verbose=False):\n",
    "    # reshapes raw data for LDA, flattening the last dimension\n",
    "    #mne.stats.permutation_t_test()\n",
    "    if verbose:\n",
    "        print('Data shape from MNE {}'.format(X.shape))\n",
    "    X_out = np.moveaxis(X,1,-1)\n",
    "    if verbose:\n",
    "        print('new data shape with sampling prioritized over channels {}'.format(X_out.shape))\n",
    "    X_out = X_out.reshape([X_out.shape[0],X_out.shape[1]*X_out.shape[2]],order='C')\n",
    "    if verbose:\n",
    "        print('Shape for K-fold LDA {}'.format(X_out.shape))\n",
    "    return X_out\n",
    "\n",
    "X = reshape_mne_raw_for_lda(X)\n",
    "\n",
    "# Think about splitting training sample and test samples\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d2a1d",
   "metadata": {},
   "source": [
    "### Compute k-fold LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='lsqr',shrinkage='auto')\n",
    "kf = KFold(n_splits=nb_k_splits)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "list_score = []\n",
    "list_auc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train_kf, X_test_kf = X[train_index], X[test_index]\n",
    "    y_train_kf, y_test_kf = y[train_index], y[test_index]\n",
    "    clf.fit(X_train_kf, y_train_kf)\n",
    "    kscore = clf.score(X_test_kf,y_test_kf)\n",
    "    y_pred_kf = clf.predict(X_test_kf)\n",
    "    k_auc = roc_auc_score(y_test_kf, y_pred_kf)\n",
    "    print('fold score: {}, AUC={}'.format(np.round(kscore, decimals=3), np.round(k_auc, decimals=3)))\n",
    "    list_score.append(kscore)\n",
    "    list_auc.append(k_auc)\n",
    "    \n",
    "print('Average score {}-Fold = {}, AUC={}'.format(kf.get_n_splits(X), np.round(np.mean(list_score), decimals=2), np.round(np.mean(list_auc), decimals=2)))\n",
    "\n",
    "# using the training/validate samples, \n",
    "clf.fit(X_train, y_train)\n",
    "score  = clf.score(X_test,y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print('Score training-validation {}, AUC={}'.format(np.round(score , decimals=2), np.round(auc, decimals=2)))\n",
    "\n",
    "print('Score is only valid if classes are balanced, please check AUC instead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ad1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_targets = epochs_resampled['Target'].events.shape[0]\n",
    "nb_non_targets = epochs_resampled['NonTarget'].events.shape[0]\n",
    "\n",
    "print('Data contains {}% of Non-targets'.format(np.round(100*(nb_non_targets / (epochs_resampled.events.shape[0])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fad580",
   "metadata": {},
   "source": [
    "### Display train-test LDA classification in a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conf_matrix(y,pred):\n",
    "    cmat = metrics.confusion_matrix(y, pred)\n",
    "    cmat_norm = metrics.confusion_matrix(y, pred, \n",
    "            normalize='true')\n",
    "    ((tn, fp), (fn, tp)) = cmat\n",
    "    ((tnr,fpr),(fnr,tpr)) = cmat_norm\n",
    "    \n",
    "    plt.figure()\n",
    "    labels = ['Non-Target', 'Target']\n",
    "    sns.heatmap(cmat, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\")\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.xlabel('Ground Truth')\n",
    "    plt.ylabel('Predicted')\n",
    "    \n",
    "    # alternative using sklearn plots\n",
    "    #plt.figure()\n",
    "    #from sklearn.metrics import ConfusionMatrixDisplay\n",
    "    #cm_display = ConfusionMatrixDisplay(cmat).plot()\n",
    "    \n",
    "    return pd.DataFrame([[f'TN = {tn} (TNR = {tnr:1.2%})', \n",
    "                                f'FP = {fp} (FPR = {fpr:1.2%})'], \n",
    "                         [f'FN = {fn} (FNR = {fnr:1.2%})', \n",
    "                                f'TP = {tp} (TPR = {tpr:1.2%})']],\n",
    "            index=['True 0(Non-Target)', 'True 1(Target)'], \n",
    "            columns=['Pred 0(Non-Target)', \n",
    "                            'Pred 1(Target)'])\n",
    "conf_matrix(y_test,y_pred)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba3aed",
   "metadata": {},
   "source": [
    "## Process the ROC curve and precision recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "y_score = clf.decision_function(X_test)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr)#.plot()\n",
    "\n",
    "\n",
    "# Precision Recall Display\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_score,\n",
    "                                         pos_label=clf.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall)#.plot()\n",
    "\n",
    "# Display them side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "roc_display.plot(ax=ax1)\n",
    "ax1.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "ax1.legend(loc=\"lower right\")\n",
    "pr_display.plot(ax=ax2)\n",
    "\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_ROC')\n",
    "    fig.savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91533fb",
   "metadata": {},
   "source": [
    "## Signed R-Square plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a2cc9",
   "metadata": {},
   "source": [
    "### Use the function from Wyrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c95b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/bbci/wyrm/blob/master/wyrm/processing.py\n",
    "# Bastian Venthur for wyrm\n",
    "# Code initially from Benjamin Blankertz for bbci (Matlab)\n",
    "\n",
    "def calculate_signed_r_square_mne(epochs, classes=[0,1], classaxis=0, **kwargs):\n",
    "    \"\"\"Calculate the signed r**2 values.\n",
    "    This method calculates the signed r**2 values over the epochs of the\n",
    "    ``dat``.\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : MNE epoched data\n",
    "    classes: list, optional \n",
    "        (either int index or str for the class name of the epoch))\n",
    "    classaxis : int, optional\n",
    "        the dimension containing epochs\n",
    "    Returns\n",
    "    -------\n",
    "    signed_r_square : ndarray\n",
    "        the signed r**2 values, signed_r_square has one axis less than\n",
    "        the ``dat`` parameter, the ``classaxis`` has been removed\n",
    "    Examples\n",
    "    --------\n",
    "    >>> dat.data.shape\n",
    "    (400, 100, 64)\n",
    "    >>> r = calculate_signed_r_square(dat)\n",
    "    >>> r.shape\n",
    "    (100, 64)\n",
    "    \"\"\"\n",
    "    # TODO: explain the algorithm in the docstring and add a reference\n",
    "    # to a paper.\n",
    "    # select class 0 and 1\n",
    "    # TODO: make class 0, 1 variables\n",
    "    fv1 = epochs[classes[0]]._data\n",
    "    fv2 = epochs[classes[1]]._data\n",
    "    # number of epochs per class\n",
    "    l1 = epochs[classes[0]]._data.shape[classaxis]\n",
    "    l2 = epochs[classes[1]]._data.shape[classaxis]\n",
    "    # calculate r-value (Benjamin approved!)\n",
    "    a = (fv1.mean(axis=classaxis) - fv2.mean(axis=classaxis)) * np.sqrt(l1 * l2)\n",
    "    b = epochs._data.std(axis=classaxis) * (l1 + l2)\n",
    "    r = a / b\n",
    "    # return signed r**2\n",
    "    return np.sign(r) * np.square(r)\n",
    "\n",
    "rsq = calculate_signed_r_square_mne(epochs, classes=['Target','NonTarget'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370a0937",
   "metadata": {},
   "source": [
    "### make a pandas database to properly display electrodes and samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6788f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a pandas database to properly display electrodes and samples\n",
    "fs = epochs.info['sfreq']\n",
    "x = np.float64(list(range(rsq.shape[1])))*(1000/fs)\n",
    "x = x.round(decimals=0).astype(np.int64) + np.int64(pre_epoch*1000)\n",
    "df_rsq = pd.DataFrame(rsq, columns=x, index=epochs.info['ch_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72663ac7",
   "metadata": {},
   "source": [
    "### Plot rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb8ac6",
   "metadata": {},
   "source": [
    "note that using a larger sampling rate will smooth this figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4beac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "hm = sns.heatmap(df_rsq,linewidths=0,cmap=\"coolwarm\").set(title='Signed r-square maps Target vs Non-Target', xlabel='Time (ms)')\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_heatmap' )\n",
    "    plt.savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8339498",
   "metadata": {},
   "source": [
    "### Quickly Display a channel with max rsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = None  # <- specify the channel here or it will be selected automatically\n",
    "if picks is None:\n",
    "    ch_max, _ = np.where(rsq == np.max(rsq))\n",
    "    picks = epochs.info['ch_names'][int(ch_max)]\n",
    "\n",
    "#picks = [f'eeg{n}' for n in range(10, 15)]\n",
    "evokeds = dict(NonTarget=list(epochs['NonTarget'].iter_evoked()), \n",
    "               Target=list(epochs['Target'].iter_evoked()))\n",
    "axs = mne.viz.plot_compare_evokeds(evokeds, picks=picks)  # use combine='mean' if several electrode chosen in picks\n",
    "\n",
    "if export_figures:\n",
    "    out_name = os.path.join(fig_folder, output_name + '_best_channel')\n",
    "    axs[0].savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdce3c",
   "metadata": {},
   "source": [
    "# Extract offline analysis, using shrinkage LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc2f5c-2adf-4c0d-9cc6-10910cfef750",
   "metadata": {},
   "source": [
    "### Extract correct target pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c135b0-7b63-4d05-99b2-ee8b217f5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target(epochs, trial_nb):\n",
    "    #epoch_trial = epochs[epochs.metadata['Trial_nb'] == trial_nb]\n",
    "    assert (epochs.metadata['Trial_nb'] == trial_nb).size > 0, 'Trial number not found in metadata'\n",
    "    idx_targets = np.where(np.logical_and(epochs.metadata['Trial_nb'] == trial_nb, \n",
    "                                          epochs.metadata['is_target']))[0]\n",
    "    # extract target or target pair\n",
    "    target_pair = np.sort(np.unique(epochs.metadata.loc[idx_targets]['stim'])).astype(np.uint8)\n",
    "    # remove the stimulus pardding to give it a meaning\n",
    "    return target_pair - stimulus_padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8cb36-da5c-45ff-8d29-b42c0bcc1e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b18e0f-56b7-4cef-90cb-9d720dd84442",
   "metadata": {},
   "source": [
    "Extract ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7377074-ffdd-443a-831b-5af962425aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_cumulative(clf, X, step):\n",
    "    \"\"\"\n",
    "    Repeats a function on the first axis on the 2d data\"\"\"\n",
    "    array_out = [clf.predict(X[0:cursor_seq,:]) for cursor_seq in np.arange(0,X.shape[0], step) + step]\n",
    "    return array_out\n",
    "\n",
    "def predict_proba_cumulative(clf, X, step):\n",
    "    \"\"\"\n",
    "    Repeats a function on the first axis on the 2d data\"\"\"\n",
    "    array_out = [clf.predict_proba(X[0:cursor_seq,:]) for cursor_seq in np.arange(0,X.shape[0], step) + step]\n",
    "    return array_out\n",
    "\n",
    "def score_cumulative(clf, X, y,  step):\n",
    "    \"\"\"\n",
    "    Repeats a function on the first axis on the 2d data\"\"\"\n",
    "    array_out = [clf.score(X[0:cursor_seq,:], y[0:cursor_seq]) for cursor_seq in np.arange(0,X.shape[0], step) + step]\n",
    "    return array_out\n",
    "\n",
    "def auc_cumulative(y, y_preds_cumulative,  step):\n",
    "    array_out = []\n",
    "    for i in range(0, y.shape[0]//step):\n",
    "        y_pred = y_preds_cumulative[i]\n",
    "        y_gt = y[0:step*(i+1)]\n",
    "        roc = roc_auc_score(y_gt, y_pred)\n",
    "        array_out.append(roc)\n",
    "        \n",
    "    #one liner not working TT\n",
    "    #array_out = [roc_auc_score(y[0:cursor_seq], y_preds_cumulative[(cursor_seq//step)-1]) for cursor_seq in np.arange(0,y.shape[0], step) + step]\n",
    "    return array_out\n",
    "\n",
    "# Transform into a function\n",
    "\n",
    "def stim_from_predict_cumulative(predicted, stim_labels):\n",
    "    \"\"\"\n",
    "    predicted: takes the output of predict_proba and returns the predicted target\n",
    "    stim_labels: stimulus labels from the epochs \n",
    "    \"\"\"\n",
    "    # retrieve the target probability index\n",
    "    max_proba_class = np.argmax(predicted,axis=1)\n",
    "    if np.max(max_proba_class) == 0:\n",
    "        print('(note:LDA detected no target when classifying n={} epochs, using best target candidate instead)'.format(predicted.shape[0]))\n",
    "        max_proba_class[[np.argmax(proba_cum_rows[0][:,1])]] = 1\n",
    "    # extract probabilities\n",
    "    max_proba = np.array([predicted[idx,max_proba_class[idx]] for idx in list(range(predicted.shape[0]))])\n",
    "    # target_indices\n",
    "    target_epochs_idx = np.where(max_proba_class)\n",
    "    # extract target stimuli from the list of stimuli and put them in a table with their respective probability\n",
    "    predicted_target_stims = stim_labels[target_epochs_idx].astype(np.uint8)\n",
    "    pred_targets_table = np.vstack((predicted_target_stims, max_proba[target_epochs_idx]))\n",
    "    # sum up target probability in this table stimulis-wise (thus dealing with situations with draws)\n",
    "    potential_targets = np.unique(pred_targets_table[0,:])\n",
    "    sum_proba_targets = [np.sum(pred_targets_table[1,np.where(pred_targets_table[0,:]==target_candidate)]) for target_candidate in potential_targets]\n",
    "    pred_targets_table_reduced = np.vstack((potential_targets, sum_proba_targets))\n",
    "\n",
    "    # return the stimulation with the highest average probability\n",
    "    predicted_stim = pred_targets_table_reduced[0,np.argmax(pred_targets_table_reduced[1,:])].astype(np.uint8)\n",
    "    \n",
    "    return predicted_stim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1d598a-d1e7-4e9b-bc79-4c83daa4a016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519cb288-04f6-4c75-a173-636048978213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether we can proceed to k split\n",
    "list_trials = np.unique(epochs.metadata[\"Trial_nb\"])\n",
    "\n",
    "print(\"nb_trials:{}, nb_folds:{}\".format(list_trials.size, nb_k_splits))\n",
    "assert list_trials.size // nb_k_splits == list_trials.size / nb_k_splits, 'number of splits must be a multiple of the number of trials'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d4435-bda4-40fd-a0a4-087d4ba70c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='lsqr',shrinkage='auto')\n",
    "\n",
    "kf = KFold(n_splits=nb_k_splits)\n",
    "\n",
    "# make a table to store the scores and accuracies\n",
    "score_table = pd.DataFrame({'fold' : pd.Series([], dtype='int'),\n",
    "                            'fold_trial' : pd.Series([], dtype='int'),\n",
    "                            'n_seq' : pd.Series([], dtype='int'),\n",
    "                            'score' : pd.Series([], dtype='float'),\n",
    "                            'AUC' : pd.Series([], dtype='float'),\n",
    "                            'row_true' : pd.Series([], dtype='str'),\n",
    "                            'row_pred' : pd.Series([], dtype='str'),\n",
    "                            'col_true' : pd.Series([], dtype='str'),\n",
    "                            'col_pred' : pd.Series([], dtype='str'),\n",
    "                            'correct' : pd.Series([], dtype='int')})\n",
    "\n",
    "fold_counter = 0\n",
    "for train_index, test_index in kf.split(list_trials):\n",
    "    a = test_index\n",
    "    \n",
    "    \n",
    "    # define the training sample\n",
    "    X_train = epochs[epochs.metadata['Trial_nb'].isin(list_trials[train_index])]._data\n",
    "    y_train = epochs[epochs.metadata['Trial_nb'].isin(list_trials[train_index])].metadata['is_target'].astype(np.uint8)\n",
    "    y_train_stim = epochs[epochs.metadata['Trial_nb'].isin(list_trials[train_index])].metadata['stim']\n",
    "\n",
    "    # reshape data to enter LDA\n",
    "    X_train = reshape_mne_raw_for_lda(X_train)\n",
    "    \n",
    "    # train the LDA classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # split the tests samples for each individual trial enable P300 speller target prediction and accuracy\n",
    "    nb_test_trials_in_split = len(list_trials)\n",
    "    for i_trial in list(range(len(test_index))):\n",
    "        trial_nb = test_index[i_trial]+1\n",
    "    \n",
    "        # both row and columns for generic classification\n",
    "        epoch_test = epochs[epochs.metadata['Trial_nb'] == trial_nb]\n",
    "        X_test = epoch_test._data\n",
    "        y_test = epoch_test[epoch_test.metadata['Trial_nb'] == trial_nb].metadata['is_target'].astype(np.uint8)\n",
    "\n",
    "        # separate row and columns target detection\n",
    "        epoch_rows = epoch_test[np.where(epoch_test.metadata['is_row'])[0]]\n",
    "        \n",
    "        skip_rows = False\n",
    "        if np.where(epoch_rows.metadata['is_target']==1)[0].size == 0:\n",
    "            print('Skipping epochs with no target rows')\n",
    "            skip_rows = True\n",
    "        else:\n",
    "            X_test_rows = epoch_rows[epoch_rows.metadata['Trial_nb'] == trial_nb]._data\n",
    "            y_test_rows = epoch_rows[epoch_rows.metadata['Trial_nb'] == trial_nb].metadata['is_target'].astype(np.uint8)\n",
    "            y_test_rows_stim = epoch_rows[epoch_rows.metadata['Trial_nb'] == trial_nb].metadata['stim']\n",
    "            X_test_rows = reshape_mne_raw_for_lda(X_test_rows)  # reshape for LDA\n",
    "            \n",
    "        skip_cols = False\n",
    "        if np.where(epoch_cols.metadata['is_target']==1)[0].size == 0:\n",
    "            skip_cols = True\n",
    "            print('Skipping epochs with no target columns')\n",
    "        else:\n",
    "            epoch_cols = epoch_test[np.where(epoch_test.metadata['is_col'])[0]]\n",
    "            X_test_cols = epoch_cols[epoch_cols.metadata['Trial_nb'] == trial_nb]._data\n",
    "            y_test_cols = epoch_cols[epoch_cols.metadata['Trial_nb'] == trial_nb].metadata['is_target'].astype(np.uint8)\n",
    "            y_test_cols_stim = epoch_cols[epoch_cols.metadata['Trial_nb'] == trial_nb].metadata['stim']\n",
    "            X_test_cols = reshape_mne_raw_for_lda(X_test_cols)  # reshape\n",
    "\n",
    "\n",
    "        # reshape data to enter LDA\n",
    "        X_test = reshape_mne_raw_for_lda(X_test)\n",
    "        \n",
    "       \n",
    "\n",
    "        # predict the targets for rows, (1 to N sequences cumulative X information provided)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # update this when dealing with rows and columns to X_test_rows and X_test_cols\n",
    "        step = X_test.shape[0]//nb_seq\n",
    "\n",
    "        # score\n",
    "        score_cum = score_cumulative(clf, X_test, y_test, step) \n",
    "        # probas calculated for prediction (aggregation will lead to cumulative prediction)\n",
    "        #proba_cum = proba_cumulative(clf, X_test, y_test, step)\n",
    "        y_test_pred_cum = predict_cumulative(clf, X_test, step)\n",
    "        # area under the curve\n",
    "        auc_cum = auc_cumulative(y_test, y_test_pred_cum, step)\n",
    "\n",
    "        if not skip_rows:\n",
    "            # prediction stim_rows\n",
    "            step_rows = X_test_rows.shape[0]//nb_seq\n",
    "            proba_cum_rows = predict_proba_cumulative(clf, X_test_rows, step_rows)\n",
    "            pred_stim_rows_cum = np.array([stim_from_predict_cumulative(proba_cum_rows[i], epoch_rows.metadata['stim'].to_numpy())\n",
    "                                  for i in list(range(len(proba_cum_rows)))])\n",
    "            true_target_rows = np.unique(epoch_rows.metadata.iloc[np.where(epoch_rows.metadata['is_target']==1)]['stim'])[0]  # does not handle k fold with several trials\n",
    "            rows_successul = pred_stim_rows_cum == true_target_rows\n",
    "            \n",
    "        if not skip_cols:\n",
    "            # columns\n",
    "            step_cols = X_test_cols.shape[0]//nb_seq\n",
    "            proba_cum_cols = predict_proba_cumulative(clf, X_test_cols, step_cols)\n",
    "            pred_stim_cols_cum = np.array([stim_from_predict_cumulative(proba_cum_cols[i], epoch_cols.metadata['stim'].to_numpy())\n",
    "                                  for i in list(range(len(proba_cum_cols)))])\n",
    "            true_target_cols = np.unique(epoch_cols.metadata.iloc[np.where(epoch_cols.metadata['is_target']==1)[0]]['stim'])[0]  # does not handle k fold with several trials\n",
    "            cols_successul = pred_stim_cols_cum == true_target_cols\n",
    "            \n",
    "            \n",
    "        if not skip_rows and not skip_cols:\n",
    "            successful_pred_cum = np.logical_and(rows_successul, cols_successul).astype(np.uint8)\n",
    "        elif skip_rows and skip_cols:\n",
    "            raise\n",
    "        elif skip_rows:\n",
    "            successful_pred_cum = cols_successul\n",
    "        elif skip_cols:\n",
    "            successful_pred_cum = rows_successul\n",
    "\n",
    "\n",
    "        # Associate predicted targets to stimuli\n",
    "        for i in range(len(score_cum)):\n",
    "            line = dict(zip(score_table.columns, [fold_counter+1, i_trial+1, i+1, # fold, fold_trial, nb of sequences\n",
    "                                                  score_cum[i], \n",
    "                                                  auc_cum[i],true_target_rows, pred_stim_rows_cum[i],\n",
    "                                                  true_target_cols, pred_stim_cols_cum[i], successful_pred_cum[i]]))\n",
    "\n",
    "            score_table = score_table.append(line, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "        print('fold {} partial score: {}, AUC={}'.format(fold_counter, np.round(score_cum[-1], decimals=3), np.round(auc_cum[-1], decimals=3)))\n",
    "\n",
    "        #print('----row score: {}, AUC={}'.format(np.round(kscore_rows, decimals=3), np.round(auc_rows, decimals=3)))\n",
    "        #print('----col score: {}, AUC={}'.format(np.round(kscore_cols, decimals=3), np.round(auc_cols, decimals=3)))\n",
    "    fold_counter += 1\n",
    "    \n",
    "# clear the types or the score table\n",
    "score_table.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2fe65-61f5-4295-87ff-5b75ac945a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(epoch_rows.metadata['is_target']==1)[0].size == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4bbcd-c843-49de-b595-8e66e9dd2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a473d2-e780-4526-ad4c-91469508b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb8e35-35a0-4b8e-a6f0-96c3c49ed9eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_seq = score_table.groupby(['n_seq']).mean()\n",
    "df_seq = df_seq.rename(columns={\"correct\":\"Accuracy\", \"score\":\"epoch_score\", \"AUC\":\"epoch_AUC\"})\n",
    "df_seq[['Accuracy','epoch_score', 'epoch_AUC']]\n",
    "\n",
    "df_seq.plot(y='Accuracy')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.suptitle('Cross-fold offline accuracy (n={})'.format(nb_k_splits))\n",
    "plt.xlabel('Number of sequences')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['P300 target prediction (row x col)'])\n",
    "# export the figure\n",
    "out_name = os.path.join(fig_folder, output_name + '_accuracy')\n",
    "plt.savefig(out_name, dpi=300, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Number of ERP targets={}, non-targets={}\".format(epochs['Target']._data.shape[0], epochs['NonTarget']._data.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a799d34-1094-4dbd-a396-306b18fe7f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdfcd28-830f-4440-850c-3ab6eee10c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
